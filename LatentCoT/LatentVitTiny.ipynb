{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b54d51",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ed7aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Haseeb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "from model import LatenViTtiny\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from PIL import Image\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8966fc6",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fdcfdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config:\n",
    "    MODEL_NAME   = 'tiny_vit_21m_224.dist_in22k_ft_in1k'\n",
    "    NUM_CLASSES  = 7      \n",
    "    NREPEAT      = 2\n",
    "    stage = 2\n",
    "    \n",
    "    BATCH_SIZE   = 128\n",
    "    NUM_EPOCHS   = 5\n",
    "    LEARNING_RATE= 1e-4\n",
    "    DEVICE       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    DATA_ROOT    = \"../../../pacs_data/pacs_data\"\n",
    "    DOMAINS      = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n",
    "    \n",
    "    TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07547270",
   "metadata": {},
   "source": [
    "### PACS Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d5a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PACSDataset(Dataset):\n",
    "    def __init__(self, root_dir, domain, transform=None):\n",
    "        self.root_dir    = os.path.join(root_dir, domain)\n",
    "        self.transform   = transform\n",
    "        self.classes     = sorted(os.listdir(self.root_dir))\n",
    "        self.class_to_idx= {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.images      = []\n",
    "        self.labels      = []\n",
    "        \n",
    "        for cls_name in self.classes:\n",
    "            cls_dir = os.path.join(self.root_dir, cls_name)\n",
    "            for img_name in os.listdir(cls_dir):\n",
    "                self.images.append(os.path.join(cls_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[cls_name])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image    = Image.open(img_path).convert('RGB')\n",
    "        label    = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fe33e",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1af0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model():\n",
    "    base_model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n",
    "    model = LatenViTtiny(\n",
    "        model     = base_model,\n",
    "        nrepeat   = Config.NREPEAT,\n",
    "        stage = Config.stage\n",
    "    )\n",
    "    return model.to(Config.DEVICE)\n",
    "\n",
    "def setup_baseline_model():\n",
    "    base_model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n",
    "    base_model.head = nn.Linear(base_model.head.in_features, Config.NUM_CLASSES)\n",
    "    return base_model.to(Config.DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db40587",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "489747cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct      = 0\n",
    "    total        = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss    = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted  = outputs.max(1)\n",
    "        total        += labels.size(0)\n",
    "        correct      += predicted.eq(labels).sum().item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc  = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d5b1e",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ee5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "        outputs        = model(images)\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total       += labels.size(0)\n",
    "        correct     += predicted.eq(labels).sum().item()\n",
    "        \n",
    "    return 100.0 * correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7a5ba",
   "metadata": {},
   "source": [
    "### Baseline -CotFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d57310d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] Train Loss: 3.7481, Train Acc: 14.11%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, Config.NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     loss, acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig.NUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m test_acc = evaluate(model, full_test_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer)\u001b[39m\n\u001b[32m     11\u001b[39m outputs = model(images)\n\u001b[32m     12\u001b[39m loss    = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m optimizer.step()\n\u001b[32m     16\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Haseeb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Haseeb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Haseeb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_loaders = []\n",
    "test_loaders  = []\n",
    "for domain in Config.DOMAINS:\n",
    "    ds_train = PACSDataset(Config.DATA_ROOT, domain, Config.TRANSFORM)\n",
    "    ds_test  = PACSDataset(Config.DATA_ROOT, domain, Config.TRANSFORM)\n",
    "    train_loaders.append(DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True))\n",
    "    test_loaders .append(DataLoader(ds_test,  batch_size=Config.BATCH_SIZE, shuffle=False))\n",
    "\n",
    "\n",
    "full_train_ds = ConcatDataset([dl.dataset for dl in train_loaders])\n",
    "full_test_ds  = ConcatDataset([dl.dataset  for dl in test_loaders ])\n",
    "full_train_loader = DataLoader(full_train_ds, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "full_test_loader  = DataLoader(full_test_ds,  batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model     = setup_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "for epoch in range(1, Config.NUM_EPOCHS + 1):\n",
    "    loss, acc = train_epoch(model, full_train_loader, criterion, optimizer)\n",
    "    print(f\"[Epoch {epoch}/{Config.NUM_EPOCHS}] Train Loss: {loss:.4f}, Train Acc: {acc:.2f}%\")\n",
    "\n",
    "test_acc = evaluate(model, full_test_loader)\n",
    "print(f\"Baseline (all domains) Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n[Per-Domain Evaluation]\")\n",
    "domain_accuracies = OrderedDict()\n",
    "for domain, loader in zip(Config.DOMAINS, test_loaders):\n",
    "    acc = evaluate(model, loader)\n",
    "    domain_accuracies[domain] = acc\n",
    "    print(f\"  {domain:>12}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = Config.TRANSFORM  \n",
    "\n",
    "lodo_results = OrderedDict()\n",
    "\n",
    "for test_domain in Config.DOMAINS:\n",
    "    print(f\"\\n=== LODO: Held-Out Domain = {test_domain} ===\")\n",
    "    train_loaders = []\n",
    "    for d in Config.DOMAINS:\n",
    "        if d == test_domain:\n",
    "            continue\n",
    "        ds_train = PACSDataset(Config.DATA_ROOT, d, transform)\n",
    "        train_loaders.append(DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True))\n",
    "\n",
    "    train_ds = ConcatDataset([dl.dataset for dl in train_loaders])\n",
    "    train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    ds_test    = PACSDataset(Config.DATA_ROOT, test_domain, transform)\n",
    "    test_loader = DataLoader(ds_test, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model     = setup_model()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(1, Config.NUM_EPOCHS + 1):\n",
    "        loss, acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        print(f\"[Epoch {epoch}/{Config.NUM_EPOCHS}] Train Loss: {loss:.4f}, Train Acc: {acc:.2f}%\")\n",
    "\n",
    "    test_acc = evaluate(model, test_loader)\n",
    "    lodo_results[test_domain] = test_acc\n",
    "    print(f\"--> Test Accuracy on {test_domain}: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n=== LODO Summary ===\")\n",
    "for domain, acc in lodo_results.items():\n",
    "    print(f\"{domain:>14}: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284a591",
   "metadata": {},
   "source": [
    "### Baseline Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loaders = []\n",
    "test_loaders  = []\n",
    "for domain in Config.DOMAINS:\n",
    "    ds_train = PACSDataset(Config.DATA_ROOT, domain, Config.TRANSFORM)\n",
    "    ds_test  = PACSDataset(Config.DATA_ROOT, domain, Config.TRANSFORM)\n",
    "    train_loaders.append(DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True))\n",
    "    test_loaders .append(DataLoader(ds_test,  batch_size=Config.BATCH_SIZE, shuffle=False))\n",
    "\n",
    "\n",
    "full_train_ds = ConcatDataset([dl.dataset for dl in train_loaders])\n",
    "full_test_ds  = ConcatDataset([dl.dataset  for dl in test_loaders ])\n",
    "full_train_loader = DataLoader(full_train_ds, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "full_test_loader  = DataLoader(full_test_ds,  batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model     = setup_baseline_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "for epoch in range(1, Config.NUM_EPOCHS + 1):\n",
    "    loss, acc = train_epoch(model, full_train_loader, criterion, optimizer)\n",
    "    print(f\"[Epoch {epoch}/{Config.NUM_EPOCHS}] Train Loss: {loss:.4f}, Train Acc: {acc:.2f}%\")\n",
    "\n",
    "test_acc = evaluate(model, full_test_loader)\n",
    "print(f\"Baseline (all domains) Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n[Per-Domain Evaluation]\")\n",
    "domain_accuracies = OrderedDict()\n",
    "for domain, loader in zip(Config.DOMAINS, test_loaders):\n",
    "    acc = evaluate(model, loader)\n",
    "    domain_accuracies[domain] = acc\n",
    "    print(f\"  {domain:>12}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47259f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = Config.TRANSFORM  \n",
    "\n",
    "lodo_results = OrderedDict()\n",
    "\n",
    "for test_domain in Config.DOMAINS:\n",
    "    print(f\"\\n=== LODO: Held-Out Domain = {test_domain} ===\")\n",
    "    train_loaders = []\n",
    "    for d in Config.DOMAINS:\n",
    "        if d == test_domain:\n",
    "            continue\n",
    "        ds_train = PACSDataset(Config.DATA_ROOT, d, transform)\n",
    "        train_loaders.append(DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True))\n",
    "\n",
    "    train_ds = ConcatDataset([dl.dataset for dl in train_loaders])\n",
    "    train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    ds_test    = PACSDataset(Config.DATA_ROOT, test_domain, transform)\n",
    "    test_loader = DataLoader(ds_test, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model     = setup_baseline_model()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(1, Config.NUM_EPOCHS + 1):\n",
    "        loss, acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        print(f\"[Epoch {epoch}/{Config.NUM_EPOCHS}] Train Loss: {loss:.4f}, Train Acc: {acc:.2f}%\")\n",
    "\n",
    "    test_acc = evaluate(model, test_loader)\n",
    "    lodo_results[test_domain] = test_acc\n",
    "    print(f\"--> Test Accuracy on {test_domain}: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n=== LODO Summary ===\")\n",
    "for domain, acc in lodo_results.items():\n",
    "    print(f\"{domain:>14}: {acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
