{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505fe958",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3da69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  \n",
    "from dataset import PACSDataset\n",
    "from vit_grqo import ViTGRQO, grqo_loss_from_gradients\n",
    "from encoder_decoder_vit import VisualDecoder, MultiheadAttn, DecoderAttn\n",
    "from Visual_query_heads import QueryLosses, GRQO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5524d",
   "metadata": {},
   "source": [
    "### CONFIG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129d8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Your constants\n",
    "DATA_ROOT = r\"D:\\Haseeb\\SPROJ\\PACS ViT\\pacs_data\\pacs_data\"\n",
    "DOMAINS = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n",
    "CLASSES = [\"dog\", \"elephant\", \"giraffe\", \"guitar\", \"horse\", \"house\", \"person\"]\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 7\n",
    "NUM_EPOCHS = 5\n",
    "LR = 1e-4\n",
    "TOPK = 24\n",
    "ALPHA = 2.0\n",
    "BETA = 0.5\n",
    "TAU = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dataset = load_dataset(\"flwrlabs/pacs\", split=\"train\")\n",
    "\n",
    "# os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "# for domain in DOMAINS:\n",
    "#     for cls in CLASSES:\n",
    "#         os.makedirs(f\"{DATA_ROOT}/{domain}/{cls}\", exist_ok=True)\n",
    "\n",
    "# for i, example in enumerate(dataset):\n",
    "#     domain = example[\"domain\"]  \n",
    "#     label_idx = example[\"label\"]  \n",
    "#     label = CLASSES[label_idx]\n",
    "\n",
    "#     if domain not in DOMAINS:\n",
    "#         raise ValueError(f\"Unexpected domain: {domain}. Expected one of {DOMAINS}\")\n",
    "#     if label not in CLASSES:\n",
    "#         raise ValueError(f\"Unexpected label: {label}. Expected one of {CLASSES}\")\n",
    "    \n",
    "#     image = example[\"image\"]\n",
    "#     image.save(f\"{DATA_ROOT}/{domain}/{label}/image_{i}.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdea6a",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c298e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "pacs_data = PACSDataset(DATA_ROOT, DOMAINS, TRANSFORM, BATCH_SIZE)\n",
    "ALL_DOMAINS = ['photo', 'art_painting', 'cartoon', 'sketch']\n",
    "LEAVE_OUT = 'sketch'  \n",
    "TRAIN_DOMAINS = [d for d in ALL_DOMAINS if d != LEAVE_OUT]\n",
    "VAL_DOMAIN = LEAVE_OUT\n",
    "\n",
    "train_datasets = [pacs_data.get_dataloader(domain=d, train=True) for d in TRAIN_DOMAINS]\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.ConcatDataset([d.dataset for d in train_datasets]),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation loader\n",
    "val_loader = pacs_data.get_dataloader(domain=VAL_DOMAIN, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893d9ae",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9106871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on cuda\n",
      "Hidden dim: 384, Num tokens: 32\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTModel, AutoFeatureExtractor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ViT-Small Backbone from Hugging Face ---\n",
    "vit_encoder = ViTModel.from_pretrained(\"WinKawaks/vit-small-patch16-224\")\n",
    "HIDDEN_DIM = vit_encoder.config.hidden_size  # 384\n",
    "\n",
    "# GRQO Hyperparameters ---\n",
    "NUM_HEADS = 6\n",
    "DROPOUT = 0.1\n",
    "NUM_LAYERS = 3\n",
    "DDROPOUT = 0.1\n",
    "NUM_TOKENS = 32\n",
    "TEMPERATURE = 0.1\n",
    "ALPHA = 2.0\n",
    "BETA = 0.5\n",
    "TAU = 1e-3\n",
    "LAMBDA_GRQO = 1.0\n",
    "TEACHER_EMA = 0.99\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "# GRQO Decoder ---\n",
    "grqo_model = GRQO(\n",
    "    Hidden_dim=HIDDEN_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    num_tokens=NUM_TOKENS,\n",
    "    ddropout=DDROPOUT,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    temperature=TEMPERATURE,\n",
    "    alpha=ALPHA,\n",
    "    beta=BETA,\n",
    "    tau=TAU,\n",
    "    lambda_grqo=LAMBDA_GRQO,\n",
    "    teacher_ema=TEACHER_EMA,\n",
    "    reward_proxy=\"taylor\"\n",
    ")\n",
    "\n",
    "# Full ViTGRQO Model ---\n",
    "class ViTGRQO(nn.Module):\n",
    "    def __init__(self, vit_encoder, grqo_model):\n",
    "        super().__init__()\n",
    "        self.vit = vit_encoder\n",
    "        self.grqo = grqo_model\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        # Get patch embeddings from HF ViT (exclude CLS token)\n",
    "        outputs = self.vit(pixel_values=x, output_hidden_states=True)\n",
    "        patch_tokens = outputs.last_hidden_state[:, 1:, :]  # [B, N, D] skip CLS token\n",
    "        return self.grqo(patch_tokens, labels)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Hidden dim: {HIDDEN_DIM}, Num tokens: {NUM_TOKENS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7e896",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da2f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_grqo_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(images, labels)\n",
    "        \n",
    "        # Extract losses\n",
    "        loss = output['loss']\n",
    "        cls_loss = output['cls_loss']\n",
    "        grqo_loss = output['grqo_loss']\n",
    "        preds = output['preds']\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        total_cls_loss += cls_loss.item() * images.size(0)\n",
    "        total_grqo_loss += grqo_loss.item() * images.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}, '\n",
    "                  f'Cls: {cls_loss.item():.4f}, GRQO: {grqo_loss.item():.4f}')\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_cls_loss = total_cls_loss / total_samples\n",
    "    avg_grqo_loss = total_grqo_loss / total_samples\n",
    "    accuracy = correct / total_samples\n",
    "    \n",
    "    return avg_loss, avg_cls_loss, avg_grqo_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dbc43",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c48930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # GRQO needs gradients even during validation\n",
    "        with torch.set_grad_enabled(True):\n",
    "            output = model(images, labels)\n",
    "            preds = output['preds']\n",
    "        \n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3fb6ed",
   "metadata": {},
   "source": [
    "### finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a010c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LODO: Leaving out domain 'sketch' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 2.0721, Cls: 2.0182, GRQO: 0.0539\n",
      "Batch 50, Loss: 0.1220, Cls: 0.1178, GRQO: 0.0042\n",
      "Batch 100, Loss: 0.1783, Cls: 0.1748, GRQO: 0.0035\n",
      "Batch 150, Loss: 0.1105, Cls: 0.1079, GRQO: 0.0026\n",
      "Train - Loss: 0.3089, Cls: 0.3027, GRQO: 0.0061, Acc: 0.9033\n",
      "Val Acc (sketch): 0.5763\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.0794, Cls: 0.0763, GRQO: 0.0030\n",
      "Batch 50, Loss: 0.0141, Cls: 0.0118, GRQO: 0.0023\n",
      "Batch 100, Loss: 0.0885, Cls: 0.0863, GRQO: 0.0022\n",
      "Batch 150, Loss: 0.0059, Cls: 0.0040, GRQO: 0.0019\n",
      "Train - Loss: 0.0640, Cls: 0.0616, GRQO: 0.0023, Acc: 0.9816\n",
      "Val Acc (sketch): 0.5611\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.1164, Cls: 0.1145, GRQO: 0.0019\n",
      "Batch 50, Loss: 0.0093, Cls: 0.0074, GRQO: 0.0019\n",
      "Batch 100, Loss: 0.0059, Cls: 0.0040, GRQO: 0.0018\n",
      "Batch 150, Loss: 0.1432, Cls: 0.1414, GRQO: 0.0017\n",
      "Train - Loss: 0.0619, Cls: 0.0601, GRQO: 0.0018, Acc: 0.9837\n",
      "Val Acc (sketch): 0.4758\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.1011, Cls: 0.0993, GRQO: 0.0018\n",
      "Batch 50, Loss: 0.0667, Cls: 0.0651, GRQO: 0.0016\n",
      "Batch 100, Loss: 0.0137, Cls: 0.0122, GRQO: 0.0015\n",
      "Batch 150, Loss: 0.0152, Cls: 0.0139, GRQO: 0.0013\n",
      "Train - Loss: 0.0435, Cls: 0.0420, GRQO: 0.0015, Acc: 0.9880\n",
      "Val Acc (sketch): 0.7023\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0032, Cls: 0.0019, GRQO: 0.0013\n",
      "Batch 50, Loss: 0.0031, Cls: 0.0018, GRQO: 0.0013\n",
      "Batch 100, Loss: 0.0037, Cls: 0.0025, GRQO: 0.0012\n",
      "Batch 150, Loss: 0.0050, Cls: 0.0038, GRQO: 0.0012\n",
      "Train - Loss: 0.0151, Cls: 0.0139, GRQO: 0.0012, Acc: 0.9961\n",
      "Val Acc (sketch): 0.6132\n",
      "Best Val Acc for sketch: 0.7023\n",
      "\n",
      "=== LODO: Leaving out domain 'photo' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 0.8457, Cls: 0.8443, GRQO: 0.0014\n",
      "Batch 50, Loss: 0.2817, Cls: 0.2803, GRQO: 0.0014\n",
      "Batch 100, Loss: 0.2344, Cls: 0.2333, GRQO: 0.0011\n",
      "Batch 150, Loss: 0.0675, Cls: 0.0664, GRQO: 0.0011\n",
      "Batch 200, Loss: 0.0103, Cls: 0.0093, GRQO: 0.0010\n",
      "Train - Loss: 0.2062, Cls: 0.2050, GRQO: 0.0012, Acc: 0.9294\n",
      "Val Acc (photo): 0.9910\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.1060, Cls: 0.1050, GRQO: 0.0010\n",
      "Batch 50, Loss: 0.0106, Cls: 0.0096, GRQO: 0.0010\n",
      "Batch 100, Loss: 0.0061, Cls: 0.0052, GRQO: 0.0009\n",
      "Batch 150, Loss: 0.0231, Cls: 0.0222, GRQO: 0.0009\n",
      "Batch 200, Loss: 0.0208, Cls: 0.0199, GRQO: 0.0008\n",
      "Train - Loss: 0.0808, Cls: 0.0799, GRQO: 0.0009, Acc: 0.9722\n",
      "Val Acc (photo): 0.9731\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.0594, Cls: 0.0587, GRQO: 0.0007\n",
      "Batch 50, Loss: 0.0222, Cls: 0.0213, GRQO: 0.0009\n",
      "Batch 100, Loss: 0.0543, Cls: 0.0535, GRQO: 0.0007\n",
      "Batch 150, Loss: 0.1557, Cls: 0.1550, GRQO: 0.0007\n",
      "Batch 200, Loss: 0.0463, Cls: 0.0456, GRQO: 0.0007\n",
      "Train - Loss: 0.0775, Cls: 0.0768, GRQO: 0.0008, Acc: 0.9755\n",
      "Val Acc (photo): 0.9701\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0029, Cls: 0.0022, GRQO: 0.0007\n",
      "Batch 50, Loss: 0.1781, Cls: 0.1774, GRQO: 0.0006\n",
      "Batch 100, Loss: 0.2270, Cls: 0.2263, GRQO: 0.0007\n",
      "Batch 150, Loss: 0.1209, Cls: 0.1201, GRQO: 0.0008\n",
      "Batch 200, Loss: 0.1547, Cls: 0.1541, GRQO: 0.0006\n",
      "Train - Loss: 0.0849, Cls: 0.0842, GRQO: 0.0007, Acc: 0.9725\n",
      "Val Acc (photo): 0.9581\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0579, Cls: 0.0573, GRQO: 0.0006\n",
      "Batch 50, Loss: 0.0063, Cls: 0.0058, GRQO: 0.0005\n",
      "Batch 100, Loss: 0.0068, Cls: 0.0063, GRQO: 0.0005\n",
      "Batch 150, Loss: 0.0839, Cls: 0.0834, GRQO: 0.0005\n",
      "Batch 200, Loss: 0.0051, Cls: 0.0047, GRQO: 0.0005\n",
      "Train - Loss: 0.0355, Cls: 0.0350, GRQO: 0.0005, Acc: 0.9899\n",
      "Val Acc (photo): 0.9701\n",
      "Best Val Acc for photo: 0.9910\n",
      "\n",
      "=== LODO: Leaving out domain 'art_painting' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 0.0095, Cls: 0.0090, GRQO: 0.0005\n",
      "Batch 50, Loss: 0.0025, Cls: 0.0020, GRQO: 0.0005\n",
      "Batch 100, Loss: 0.0867, Cls: 0.0863, GRQO: 0.0004\n",
      "Batch 150, Loss: 0.1666, Cls: 0.1662, GRQO: 0.0004\n",
      "Train - Loss: 0.0641, Cls: 0.0637, GRQO: 0.0004, Acc: 0.9792\n",
      "Val Acc (art_painting): 0.8902\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.0324, Cls: 0.0320, GRQO: 0.0004\n",
      "Batch 50, Loss: 0.1533, Cls: 0.1529, GRQO: 0.0004\n",
      "Batch 100, Loss: 0.0155, Cls: 0.0152, GRQO: 0.0003\n",
      "Batch 150, Loss: 0.0129, Cls: 0.0124, GRQO: 0.0005\n",
      "Train - Loss: 0.0406, Cls: 0.0403, GRQO: 0.0004, Acc: 0.9876\n",
      "Val Acc (art_painting): 0.9146\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.1696, Cls: 0.1693, GRQO: 0.0003\n",
      "Batch 50, Loss: 0.0866, Cls: 0.0863, GRQO: 0.0003\n",
      "Batch 100, Loss: 0.0829, Cls: 0.0826, GRQO: 0.0003\n",
      "Batch 150, Loss: 0.0095, Cls: 0.0093, GRQO: 0.0002\n",
      "Train - Loss: 0.0480, Cls: 0.0477, GRQO: 0.0003, Acc: 0.9843\n",
      "Val Acc (art_painting): 0.8634\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.3025, Cls: 0.3022, GRQO: 0.0003\n",
      "Batch 50, Loss: 0.0153, Cls: 0.0151, GRQO: 0.0002\n",
      "Batch 100, Loss: 0.0021, Cls: 0.0019, GRQO: 0.0002\n",
      "Batch 150, Loss: 0.0041, Cls: 0.0039, GRQO: 0.0002\n",
      "Train - Loss: 0.0277, Cls: 0.0275, GRQO: 0.0002, Acc: 0.9915\n",
      "Val Acc (art_painting): 0.9049\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0184, Cls: 0.0182, GRQO: 0.0002\n",
      "Batch 50, Loss: 0.0861, Cls: 0.0859, GRQO: 0.0002\n",
      "Batch 100, Loss: 0.0014, Cls: 0.0012, GRQO: 0.0002\n",
      "Batch 150, Loss: 0.0110, Cls: 0.0108, GRQO: 0.0002\n",
      "Train - Loss: 0.0247, Cls: 0.0246, GRQO: 0.0002, Acc: 0.9909\n",
      "Val Acc (art_painting): 0.8902\n",
      "Best Val Acc for art_painting: 0.9146\n",
      "\n",
      "=== LODO: Leaving out domain 'cartoon' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 0.1891, Cls: 0.1889, GRQO: 0.0002\n",
      "Batch 50, Loss: 0.0022, Cls: 0.0021, GRQO: 0.0002\n",
      "Batch 100, Loss: 0.2812, Cls: 0.2809, GRQO: 0.0002\n",
      "Batch 150, Loss: 0.0681, Cls: 0.0679, GRQO: 0.0002\n",
      "Train - Loss: 0.0552, Cls: 0.0550, GRQO: 0.0002, Acc: 0.9838\n",
      "Val Acc (cartoon): 0.9403\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.0056, Cls: 0.0054, GRQO: 0.0002\n",
      "Batch 50, Loss: 0.1084, Cls: 0.1083, GRQO: 0.0002\n",
      "Batch 100, Loss: 0.1681, Cls: 0.1679, GRQO: 0.0001\n",
      "Batch 150, Loss: 0.0008, Cls: 0.0006, GRQO: 0.0001\n",
      "Train - Loss: 0.0260, Cls: 0.0259, GRQO: 0.0002, Acc: 0.9925\n",
      "Val Acc (cartoon): 0.9339\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.0007, Cls: 0.0006, GRQO: 0.0001\n",
      "Batch 50, Loss: 0.0490, Cls: 0.0489, GRQO: 0.0001\n",
      "Batch 100, Loss: 0.0011, Cls: 0.0010, GRQO: 0.0001\n",
      "Batch 150, Loss: 0.1185, Cls: 0.1183, GRQO: 0.0002\n",
      "Train - Loss: 0.0362, Cls: 0.0360, GRQO: 0.0002, Acc: 0.9902\n",
      "Val Acc (cartoon): 0.9424\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0017, Cls: 0.0016, GRQO: 0.0001\n",
      "Batch 50, Loss: 0.1237, Cls: 0.1235, GRQO: 0.0002\n",
      "Batch 100, Loss: 0.0230, Cls: 0.0228, GRQO: 0.0002\n",
      "Batch 150, Loss: 0.0118, Cls: 0.0117, GRQO: 0.0002\n",
      "Train - Loss: 0.0644, Cls: 0.0642, GRQO: 0.0002, Acc: 0.9805\n",
      "Val Acc (cartoon): 0.9168\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0035, Cls: 0.0033, GRQO: 0.0002\n",
      "Batch 50, Loss: 0.0015, Cls: 0.0013, GRQO: 0.0001\n",
      "Batch 100, Loss: 0.1576, Cls: 0.1575, GRQO: 0.0001\n",
      "Batch 150, Loss: 0.0023, Cls: 0.0022, GRQO: 0.0001\n",
      "Train - Loss: 0.0315, Cls: 0.0314, GRQO: 0.0001, Acc: 0.9905\n",
      "Val Acc (cartoon): 0.9062\n",
      "Best Val Acc for cartoon: 0.9424\n",
      "\n",
      "==================================================\n",
      "LODO RESULTS SUMMARY\n",
      "==================================================\n",
      "sketch         : 0.7023\n",
      "photo          : 0.9910\n",
      "art_painting   : 0.9146\n",
      "cartoon        : 0.9424\n",
      "Average        : 0.8876\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "ALL_DOMAINS = ['sketch', 'photo', 'art_painting', 'cartoon']\n",
    "lodo_results = {}\n",
    "\n",
    "for LEAVE_OUT in ALL_DOMAINS:\n",
    "    print(f\"\\n=== LODO: Leaving out domain '{LEAVE_OUT}' ===\")\n",
    "    TRAIN_DOMAINS = [d for d in ALL_DOMAINS if d != LEAVE_OUT]\n",
    "    VAL_DOMAIN = LEAVE_OUT\n",
    "\n",
    "    # Create data loaders\n",
    "    train_datasets = [pacs_data.get_dataloader(domain=d, train=True) for d in TRAIN_DOMAINS]\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.ConcatDataset([d.dataset for d in train_datasets]),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_loader = pacs_data.get_dataloader(domain=VAL_DOMAIN, train=False)\n",
    "\n",
    "    # Reset model for this split\n",
    "    model = ViTGRQO(vit_encoder, grqo_model).to(DEVICE)\n",
    "    FREEZE_VIT=False\n",
    "    if FREEZE_VIT:\n",
    "        for param in model.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "        # Optimizer ---\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=LR, weight_decay=0.01)\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_cls, train_grqo, train_acc = train_epoch(model, train_loader, optimizer, DEVICE)\n",
    "        \n",
    "        # Validate  \n",
    "        val_acc = validate(model, val_loader, DEVICE)\n",
    "        \n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Cls: {train_cls:.4f}, \"\n",
    "              f\"GRQO: {train_grqo:.4f}, Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Acc ({VAL_DOMAIN}): {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    lodo_results[VAL_DOMAIN] = best_val_acc\n",
    "    print(f\"Best Val Acc for {VAL_DOMAIN}: {best_val_acc:.4f}\")\n",
    "\n",
    "# Cell 8: Results Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LODO RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for domain, acc in lodo_results.items():\n",
    "    print(f\"{domain:15}: {acc:.4f}\")\n",
    "\n",
    "avg_lodo = sum(lodo_results.values()) / len(lodo_results)\n",
    "print(f\"{'Average':15}: {avg_lodo:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0913f84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LODO: Leaving out domain 'sketch' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 1.9759, Cls: 1.9270, GRQO: 0.0489\n",
      "Batch 50, Loss: 0.3641, Cls: 0.3533, GRQO: 0.0109\n",
      "Batch 100, Loss: 0.1038, Cls: 0.0993, GRQO: 0.0045\n",
      "Batch 150, Loss: 0.2622, Cls: 0.2580, GRQO: 0.0042\n",
      "Train - Loss: 0.4285, Cls: 0.4200, GRQO: 0.0085, Acc: 0.8765\n",
      "Val Acc (sketch): 0.3969\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.1284, Cls: 0.1247, GRQO: 0.0037\n",
      "Batch 50, Loss: 0.0844, Cls: 0.0809, GRQO: 0.0034\n",
      "Batch 100, Loss: 0.1651, Cls: 0.1614, GRQO: 0.0037\n",
      "Batch 150, Loss: 0.0296, Cls: 0.0264, GRQO: 0.0032\n",
      "Train - Loss: 0.1216, Cls: 0.1180, GRQO: 0.0036, Acc: 0.9618\n",
      "Val Acc (sketch): 0.3842\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.0492, Cls: 0.0461, GRQO: 0.0030\n",
      "Batch 50, Loss: 0.1007, Cls: 0.0979, GRQO: 0.0029\n",
      "Batch 100, Loss: 0.1257, Cls: 0.1226, GRQO: 0.0031\n",
      "Batch 150, Loss: 0.0267, Cls: 0.0239, GRQO: 0.0029\n",
      "Train - Loss: 0.0601, Cls: 0.0573, GRQO: 0.0029, Acc: 0.9806\n",
      "Val Acc (sketch): 0.3728\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0850, Cls: 0.0823, GRQO: 0.0027\n",
      "Batch 50, Loss: 0.0187, Cls: 0.0162, GRQO: 0.0025\n",
      "Batch 100, Loss: 0.0104, Cls: 0.0081, GRQO: 0.0023\n",
      "Batch 150, Loss: 0.0106, Cls: 0.0080, GRQO: 0.0025\n",
      "Train - Loss: 0.0286, Cls: 0.0261, GRQO: 0.0025, Acc: 0.9924\n",
      "Val Acc (sketch): 0.3919\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0045, Cls: 0.0021, GRQO: 0.0024\n",
      "Batch 50, Loss: 0.0121, Cls: 0.0095, GRQO: 0.0026\n",
      "Batch 100, Loss: 0.0109, Cls: 0.0088, GRQO: 0.0021\n",
      "Batch 150, Loss: 0.0056, Cls: 0.0034, GRQO: 0.0022\n",
      "Train - Loss: 0.0098, Cls: 0.0076, GRQO: 0.0022, Acc: 0.9988\n",
      "Val Acc (sketch): 0.3779\n",
      "Best Val Acc for sketch: 0.3969\n",
      "\n",
      "=== LODO: Leaving out domain 'photo' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 1.9707, Cls: 1.8991, GRQO: 0.0715\n",
      "Batch 50, Loss: 0.6209, Cls: 0.6138, GRQO: 0.0072\n",
      "Batch 100, Loss: 0.5557, Cls: 0.5491, GRQO: 0.0065\n",
      "Batch 150, Loss: 0.4079, Cls: 0.4026, GRQO: 0.0053\n",
      "Batch 200, Loss: 0.5976, Cls: 0.5930, GRQO: 0.0046\n",
      "Train - Loss: 0.6423, Cls: 0.6347, GRQO: 0.0076, Acc: 0.7806\n",
      "Val Acc (photo): 0.9910\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.3265, Cls: 0.3221, GRQO: 0.0044\n",
      "Batch 50, Loss: 0.1633, Cls: 0.1594, GRQO: 0.0039\n",
      "Batch 100, Loss: 0.0954, Cls: 0.0919, GRQO: 0.0035\n",
      "Batch 150, Loss: 0.1649, Cls: 0.1613, GRQO: 0.0036\n",
      "Batch 200, Loss: 0.3426, Cls: 0.3395, GRQO: 0.0031\n",
      "Train - Loss: 0.2804, Cls: 0.2767, GRQO: 0.0037, Acc: 0.9023\n",
      "Val Acc (photo): 0.9850\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.1816, Cls: 0.1782, GRQO: 0.0034\n",
      "Batch 50, Loss: 0.2601, Cls: 0.2571, GRQO: 0.0030\n",
      "Batch 100, Loss: 0.3011, Cls: 0.2980, GRQO: 0.0031\n",
      "Batch 150, Loss: 0.0538, Cls: 0.0504, GRQO: 0.0034\n",
      "Batch 200, Loss: 0.3761, Cls: 0.3731, GRQO: 0.0030\n",
      "Train - Loss: 0.1826, Cls: 0.1794, GRQO: 0.0031, Acc: 0.9372\n",
      "Val Acc (photo): 0.9551\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.1009, Cls: 0.0981, GRQO: 0.0028\n",
      "Batch 50, Loss: 0.0842, Cls: 0.0814, GRQO: 0.0028\n",
      "Batch 100, Loss: 0.1811, Cls: 0.1782, GRQO: 0.0029\n",
      "Batch 150, Loss: 0.0766, Cls: 0.0739, GRQO: 0.0028\n",
      "Batch 200, Loss: 0.0983, Cls: 0.0954, GRQO: 0.0029\n",
      "Train - Loss: 0.1119, Cls: 0.1090, GRQO: 0.0029, Acc: 0.9651\n",
      "Val Acc (photo): 0.9701\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0511, Cls: 0.0484, GRQO: 0.0027\n",
      "Batch 50, Loss: 0.0387, Cls: 0.0362, GRQO: 0.0024\n",
      "Batch 100, Loss: 0.0140, Cls: 0.0114, GRQO: 0.0026\n",
      "Batch 150, Loss: 0.0134, Cls: 0.0109, GRQO: 0.0026\n",
      "Batch 200, Loss: 0.0783, Cls: 0.0758, GRQO: 0.0024\n",
      "Train - Loss: 0.0539, Cls: 0.0513, GRQO: 0.0026, Acc: 0.9854\n",
      "Val Acc (photo): 0.9671\n",
      "Best Val Acc for photo: 0.9910\n",
      "\n",
      "=== LODO: Leaving out domain 'art_painting' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 2.0875, Cls: 1.9822, GRQO: 0.1053\n",
      "Batch 50, Loss: 0.6611, Cls: 0.6532, GRQO: 0.0079\n",
      "Batch 100, Loss: 0.2723, Cls: 0.2659, GRQO: 0.0064\n",
      "Batch 150, Loss: 0.4535, Cls: 0.4482, GRQO: 0.0053\n",
      "Train - Loss: 0.5997, Cls: 0.5912, GRQO: 0.0086, Acc: 0.7965\n",
      "Val Acc (art_painting): 0.8341\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.1724, Cls: 0.1679, GRQO: 0.0045\n",
      "Batch 50, Loss: 0.4125, Cls: 0.4087, GRQO: 0.0038\n",
      "Batch 100, Loss: 0.2649, Cls: 0.2609, GRQO: 0.0040\n",
      "Batch 150, Loss: 0.2352, Cls: 0.2317, GRQO: 0.0035\n",
      "Train - Loss: 0.2568, Cls: 0.2530, GRQO: 0.0038, Acc: 0.9067\n",
      "Val Acc (art_painting): 0.8512\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.0605, Cls: 0.0573, GRQO: 0.0032\n",
      "Batch 50, Loss: 0.1783, Cls: 0.1752, GRQO: 0.0031\n",
      "Batch 100, Loss: 0.2864, Cls: 0.2832, GRQO: 0.0032\n",
      "Batch 150, Loss: 0.2197, Cls: 0.2164, GRQO: 0.0032\n",
      "Train - Loss: 0.1690, Cls: 0.1658, GRQO: 0.0032, Acc: 0.9396\n",
      "Val Acc (art_painting): 0.8610\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0986, Cls: 0.0957, GRQO: 0.0028\n",
      "Batch 50, Loss: 0.1632, Cls: 0.1604, GRQO: 0.0029\n",
      "Batch 100, Loss: 0.0852, Cls: 0.0821, GRQO: 0.0032\n",
      "Batch 150, Loss: 0.0144, Cls: 0.0116, GRQO: 0.0028\n",
      "Train - Loss: 0.0993, Cls: 0.0963, GRQO: 0.0030, Acc: 0.9681\n",
      "Val Acc (art_painting): 0.8561\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0654, Cls: 0.0623, GRQO: 0.0031\n",
      "Batch 50, Loss: 0.0664, Cls: 0.0638, GRQO: 0.0026\n",
      "Batch 100, Loss: 0.0424, Cls: 0.0397, GRQO: 0.0026\n",
      "Batch 150, Loss: 0.1229, Cls: 0.1204, GRQO: 0.0025\n",
      "Train - Loss: 0.0578, Cls: 0.0551, GRQO: 0.0027, Acc: 0.9819\n",
      "Val Acc (art_painting): 0.8756\n",
      "Best Val Acc for art_painting: 0.8756\n",
      "\n",
      "=== LODO: Leaving out domain 'cartoon' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 1.9982, Cls: 1.9123, GRQO: 0.0859\n",
      "Batch 50, Loss: 0.4416, Cls: 0.4340, GRQO: 0.0076\n",
      "Batch 100, Loss: 0.2336, Cls: 0.2284, GRQO: 0.0052\n",
      "Batch 150, Loss: 0.3091, Cls: 0.3050, GRQO: 0.0041\n",
      "Train - Loss: 0.5776, Cls: 0.5701, GRQO: 0.0076, Acc: 0.8058\n",
      "Val Acc (cartoon): 0.6503\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.3899, Cls: 0.3856, GRQO: 0.0043\n",
      "Batch 50, Loss: 0.3326, Cls: 0.3290, GRQO: 0.0036\n",
      "Batch 100, Loss: 0.1211, Cls: 0.1177, GRQO: 0.0034\n",
      "Batch 150, Loss: 0.0658, Cls: 0.0625, GRQO: 0.0032\n",
      "Train - Loss: 0.2294, Cls: 0.2260, GRQO: 0.0034, Acc: 0.9174\n",
      "Val Acc (cartoon): 0.7420\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.2090, Cls: 0.2058, GRQO: 0.0032\n",
      "Batch 50, Loss: 0.1360, Cls: 0.1328, GRQO: 0.0032\n",
      "Batch 100, Loss: 0.1734, Cls: 0.1708, GRQO: 0.0026\n",
      "Batch 150, Loss: 0.1257, Cls: 0.1231, GRQO: 0.0026\n",
      "Train - Loss: 0.1449, Cls: 0.1421, GRQO: 0.0028, Acc: 0.9501\n",
      "Val Acc (cartoon): 0.7356\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.1344, Cls: 0.1318, GRQO: 0.0026\n",
      "Batch 50, Loss: 0.1200, Cls: 0.1174, GRQO: 0.0026\n",
      "Batch 100, Loss: 0.1502, Cls: 0.1479, GRQO: 0.0024\n",
      "Batch 150, Loss: 0.0363, Cls: 0.0339, GRQO: 0.0024\n",
      "Train - Loss: 0.0808, Cls: 0.0783, GRQO: 0.0026, Acc: 0.9761\n",
      "Val Acc (cartoon): 0.7015\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0834, Cls: 0.0811, GRQO: 0.0024\n",
      "Batch 50, Loss: 0.0139, Cls: 0.0116, GRQO: 0.0022\n",
      "Batch 100, Loss: 0.0403, Cls: 0.0380, GRQO: 0.0022\n",
      "Batch 150, Loss: 0.0058, Cls: 0.0035, GRQO: 0.0023\n",
      "Train - Loss: 0.0474, Cls: 0.0450, GRQO: 0.0024, Acc: 0.9850\n",
      "Val Acc (cartoon): 0.7164\n",
      "Best Val Acc for cartoon: 0.7420\n",
      "\n",
      "==================================================\n",
      "LODO RESULTS SUMMARY\n",
      "==================================================\n",
      "sketch         : 0.3969\n",
      "photo          : 0.9910\n",
      "art_painting   : 0.8756\n",
      "cartoon        : 0.7420\n",
      "Average        : 0.7514\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "ALL_DOMAINS = ['sketch', 'photo', 'art_painting', 'cartoon']\n",
    "lodo_results = {}\n",
    "\n",
    "for LEAVE_OUT in ALL_DOMAINS:\n",
    "    print(f\"\\n=== LODO: Leaving out domain '{LEAVE_OUT}' ===\")\n",
    "    TRAIN_DOMAINS = [d for d in ALL_DOMAINS if d != LEAVE_OUT]\n",
    "    VAL_DOMAIN = LEAVE_OUT\n",
    "\n",
    "    # Create data loaders\n",
    "    train_datasets = [pacs_data.get_dataloader(domain=d, train=True) for d in TRAIN_DOMAINS]\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.ConcatDataset([d.dataset for d in train_datasets]),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_loader = pacs_data.get_dataloader(domain=VAL_DOMAIN, train=False)\n",
    "    vit_encoder = ViTModel.from_pretrained(\"WinKawaks/vit-small-patch16-224\")\n",
    "    # Reset model for this split\n",
    "    grqo_model = GRQO(\n",
    "        Hidden_dim=HIDDEN_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        num_tokens=NUM_TOKENS,\n",
    "        ddropout=DDROPOUT,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        temperature=TEMPERATURE,\n",
    "        alpha=ALPHA,\n",
    "        beta=BETA,\n",
    "        tau=TAU,\n",
    "        lambda_grqo=LAMBDA_GRQO,\n",
    "        teacher_ema=TEACHER_EMA,\n",
    "        reward_proxy=\"taylor\"\n",
    "    )\n",
    "    model = ViTGRQO(vit_encoder, grqo_model).to(DEVICE)\n",
    "    FREEZE_VIT=True\n",
    "    if FREEZE_VIT:\n",
    "        for param in model.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=LR, weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_cls, train_grqo, train_acc = train_epoch(model, train_loader, optimizer, DEVICE)\n",
    "        \n",
    "        # Validate  \n",
    "        val_acc = validate(model, val_loader, DEVICE)\n",
    "        \n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Cls: {train_cls:.4f}, \"\n",
    "              f\"GRQO: {train_grqo:.4f}, Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Acc ({VAL_DOMAIN}): {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    lodo_results[VAL_DOMAIN] = best_val_acc\n",
    "    print(f\"Best Val Acc for {VAL_DOMAIN}: {best_val_acc:.4f}\")\n",
    "\n",
    "# Cell 8: Results Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LODO RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for domain, acc in lodo_results.items():\n",
    "    print(f\"{domain:15}: {acc:.4f}\")\n",
    "\n",
    "avg_lodo = sum(lodo_results.values()) / len(lodo_results)\n",
    "print(f\"{'Average':15}: {avg_lodo:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1676ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Leave-One-Domain-Out (LODO) Training & Validation ===\n",
      "\n",
      "=== LODO: Leaving out SKETCH for validation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([7, 384]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5 (Leave out sketch)\n",
      "Train - Loss: 0.2360, Acc: 0.9227\n",
      "Val Acc (sketch): 0.4402\n",
      "\n",
      "Epoch 2/5 (Leave out sketch)\n",
      "Train - Loss: 0.0203, Acc: 0.9953\n",
      "Val Acc (sketch): 0.5115\n",
      "\n",
      "Epoch 3/5 (Leave out sketch)\n",
      "Train - Loss: 0.0131, Acc: 0.9961\n",
      "Val Acc (sketch): 0.5076\n",
      "\n",
      "Epoch 4/5 (Leave out sketch)\n",
      "Train - Loss: 0.0500, Acc: 0.9854\n",
      "Val Acc (sketch): 0.5102\n",
      "\n",
      "Epoch 5/5 (Leave out sketch)\n",
      "Train - Loss: 0.0344, Acc: 0.9897\n",
      "Val Acc (sketch): 0.6285\n",
      "\n",
      "=== LODO: Leaving out PHOTO for validation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([7, 384]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5 (Leave out photo)\n",
      "Train - Loss: 0.3381, Acc: 0.8800\n",
      "Val Acc (photo): 0.9731\n",
      "\n",
      "Epoch 2/5 (Leave out photo)\n",
      "Train - Loss: 0.0767, Acc: 0.9731\n",
      "Val Acc (photo): 0.9731\n",
      "\n",
      "Epoch 3/5 (Leave out photo)\n",
      "Train - Loss: 0.0467, Acc: 0.9833\n",
      "Val Acc (photo): 0.9701\n",
      "\n",
      "Epoch 4/5 (Leave out photo)\n",
      "Train - Loss: 0.0414, Acc: 0.9857\n",
      "Val Acc (photo): 0.9611\n",
      "\n",
      "Epoch 5/5 (Leave out photo)\n",
      "Train - Loss: 0.0402, Acc: 0.9872\n",
      "Val Acc (photo): 0.9341\n",
      "\n",
      "=== LODO: Leaving out ART_PAINTING for validation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([7, 384]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5 (Leave out art_painting)\n",
      "Train - Loss: 0.3617, Acc: 0.8664\n",
      "Val Acc (art_painting): 0.8561\n",
      "\n",
      "Epoch 2/5 (Leave out art_painting)\n",
      "Train - Loss: 0.0735, Acc: 0.9753\n",
      "Val Acc (art_painting): 0.8634\n",
      "\n",
      "Epoch 3/5 (Leave out art_painting)\n",
      "Train - Loss: 0.0303, Acc: 0.9898\n",
      "Val Acc (art_painting): 0.8707\n",
      "\n",
      "Epoch 4/5 (Leave out art_painting)\n",
      "Train - Loss: 0.0276, Acc: 0.9899\n",
      "Val Acc (art_painting): 0.7878\n",
      "\n",
      "Epoch 5/5 (Leave out art_painting)\n",
      "Train - Loss: 0.0571, Acc: 0.9778\n",
      "Val Acc (art_painting): 0.8439\n",
      "\n",
      "=== LODO: Leaving out CARTOON for validation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([7, 384]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5 (Leave out cartoon)\n",
      "Train - Loss: 0.3680, Acc: 0.8622\n",
      "Val Acc (cartoon): 0.7996\n",
      "\n",
      "Epoch 2/5 (Leave out cartoon)\n",
      "Train - Loss: 0.0893, Acc: 0.9684\n",
      "Val Acc (cartoon): 0.7910\n",
      "\n",
      "Epoch 3/5 (Leave out cartoon)\n",
      "Train - Loss: 0.0418, Acc: 0.9851\n",
      "Val Acc (cartoon): 0.7996\n",
      "\n",
      "Epoch 4/5 (Leave out cartoon)\n",
      "Train - Loss: 0.0204, Acc: 0.9935\n",
      "Val Acc (cartoon): 0.7910\n",
      "\n",
      "Epoch 5/5 (Leave out cartoon)\n",
      "Train - Loss: 0.0634, Acc: 0.9804\n",
      "Val Acc (cartoon): 0.7591\n",
      "\n",
      "==================================================\n",
      "LODO RESULTS SUMMARY\n",
      "==================================================\n",
      "sketch         : 0.6285\n",
      "photo          : 0.9731\n",
      "art_painting   : 0.8707\n",
      "cartoon        : 0.7996\n",
      "Average LODO   : 0.8180\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Leave-One-Domain-Out (LODO) Training & Validation ===\")\n",
    "\n",
    "lodo_results = {}  # store per-domain results\n",
    "\n",
    "for LEAVE_OUT in ALL_DOMAINS:\n",
    "    print(f\"\\n=== LODO: Leaving out {LEAVE_OUT.upper()} for validation ===\")\n",
    "    \n",
    "    TRAIN_DOMAINS = [d for d in ALL_DOMAINS if d != LEAVE_OUT]\n",
    "    VAL_DOMAIN = LEAVE_OUT\n",
    "\n",
    "    # Train loader (concat all train domains)\n",
    "    train_datasets = [pacs_data.get_dataloader(domain=d, train=True) for d in TRAIN_DOMAINS]\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.ConcatDataset([d.dataset for d in train_datasets]),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Validation loader (only leave-out domain)\n",
    "    val_loader = pacs_data.get_dataloader(domain=VAL_DOMAIN, train=False)\n",
    "\n",
    "    # Model (reset for each LODO run)\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        \"WinKawaks/vit-small-patch16-224\",\n",
    "        num_labels=NUM_CLASSES,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} (Leave out {VAL_DOMAIN})\")\n",
    "\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        running_loss, running_corrects, running_samples = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            running_samples += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / running_samples\n",
    "        train_acc = running_corrects / running_samples\n",
    "\n",
    "        # ---- Validation on held-out domain ----\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.logits.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        val_acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Acc ({VAL_DOMAIN}): {val_acc:.4f}\")\n",
    "\n",
    "        # Track best\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "\n",
    "    # Save best for this LODO run\n",
    "    lodo_results[VAL_DOMAIN] = best_val_acc\n",
    "\n",
    "# ---------------- Results Summary ----------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LODO RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for domain, acc in lodo_results.items():\n",
    "    print(f\"{domain:15}: {acc:.4f}\")\n",
    "\n",
    "avg_lodo = sum(lodo_results.values()) / len(lodo_results)\n",
    "print(f\"{'Average LODO':15}: {avg_lodo:.4f}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a55da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "results = {\n",
    "    \"photo\": 0.9581,\n",
    "    \"art_painting\": 0.7293,\n",
    "    \"cartoon\": 0.7186,\n",
    "    \"sketch\": 0.3461,\n",
    "    \"All-domains baseline\": 0.9010\n",
    "}\n",
    "\n",
    "# Prepare labels & values; wrap long label onto two lines for neatness\n",
    "labels = []\n",
    "values = []\n",
    "for k, v in results.items():\n",
    "    if \"All-domains\" in k:\n",
    "        labels.append(\"All-domains\\nbaseline\")   # wrap long label\n",
    "    else:\n",
    "        labels.append(k.replace(\"_\", \" \"))       # nicer display for underscores\n",
    "    values.append(v)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "x = range(len(labels))\n",
    "\n",
    "# Draw bars; make baseline visually distinct\n",
    "colors = [\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B2\", \"#888888\"]\n",
    "bars = ax.bar(x, values, color=colors, edgecolor=\"black\", linewidth=0.7)\n",
    "\n",
    "# Axis and ticks\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=11)\n",
    "ax.set_title(\"LODO Results by Domain\", fontsize=13, weight=\"bold\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, fontsize=10, rotation=0, ha=\"center\")\n",
    "\n",
    "# Add horizontal grid lines for readability (below bars)\n",
    "ax.yaxis.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Annotate values above bars with consistent alignment\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height + 0.02,\n",
    "        f\"{val:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "        fontweight=\"semibold\"\n",
    "    )\n",
    "\n",
    "# Tidy up spines\n",
    "for spine in (\"top\", \"right\"):\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save optionally:\n",
    "# fig.savefig(\"lodo_results_bar.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
