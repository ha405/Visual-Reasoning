{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505fe958",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3da69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from dataset import PACSDataset\n",
    "from vit_grqo import ViTGRQO, grqo_loss_from_gradients\n",
    "from encoder_decoder_vit import VisualDecoder, MultiheadAttn, DecoderAttn\n",
    "from Visual_query_heads import QueryLosses, GRQO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5524d",
   "metadata": {},
   "source": [
    "### CONFIG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129d8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Your constants\n",
    "DATA_ROOT = \"../../../pacs_data/pacs_data\"\n",
    "DOMAINS = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n",
    "CLASSES = [\"dog\", \"elephant\", \"giraffe\", \"guitar\", \"horse\", \"house\", \"person\"]\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 7\n",
    "NUM_EPOCHS = 5\n",
    "LR = 1e-4\n",
    "TOPK = 24\n",
    "ALPHA = 2.0\n",
    "BETA = 0.5\n",
    "TAU = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dataset = load_dataset(\"flwrlabs/pacs\", split=\"train\")\n",
    "\n",
    "# os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "# for domain in DOMAINS:\n",
    "#     for cls in CLASSES:\n",
    "#         os.makedirs(f\"{DATA_ROOT}/{domain}/{cls}\", exist_ok=True)\n",
    "\n",
    "# for i, example in enumerate(dataset):\n",
    "#     domain = example[\"domain\"]  \n",
    "#     label_idx = example[\"label\"]  \n",
    "#     label = CLASSES[label_idx]\n",
    "\n",
    "#     if domain not in DOMAINS:\n",
    "#         raise ValueError(f\"Unexpected domain: {domain}. Expected one of {DOMAINS}\")\n",
    "#     if label not in CLASSES:\n",
    "#         raise ValueError(f\"Unexpected label: {label}. Expected one of {CLASSES}\")\n",
    "    \n",
    "#     image = example[\"image\"]\n",
    "#     image.save(f\"{DATA_ROOT}/{domain}/{label}/image_{i}.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdea6a",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c298e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "pacs_data = PACSDataset(DATA_ROOT, DOMAINS, TRANSFORM, BATCH_SIZE)\n",
    "ALL_DOMAINS = ['photo', 'art_painting', 'cartoon', 'sketch']\n",
    "LEAVE_OUT = 'sketch'  \n",
    "TRAIN_DOMAINS = [d for d in ALL_DOMAINS if d != LEAVE_OUT]\n",
    "VAL_DOMAIN = LEAVE_OUT\n",
    "\n",
    "train_datasets = [pacs_data.get_dataloader(domain=d, train=True) for d in TRAIN_DOMAINS]\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.ConcatDataset([d.dataset for d in train_datasets]),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation loader\n",
    "val_loader = pacs_data.get_dataloader(domain=VAL_DOMAIN, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893d9ae",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9106871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT backbone trainable\n",
      "Model initialized on cuda\n",
      "Hidden dim: 192, Num tokens: 32\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTModel, AutoFeatureExtractor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- 1) ViT-Tiny Backbone from Hugging Face ---\n",
    "vit_encoder = ViTModel.from_pretrained(\"WinKawaks/vit-tiny-patch16-224\")\n",
    "HIDDEN_DIM = vit_encoder.config.hidden_size  # 192 for ViT-Tiny\n",
    "\n",
    "# --- 2) GRQO Hyperparameters ---\n",
    "NUM_HEADS = 6\n",
    "DROPOUT = 0.1\n",
    "NUM_LAYERS = 3\n",
    "DDROPOUT = 0.1\n",
    "NUM_TOKENS = 32\n",
    "TEMPERATURE = 0.1\n",
    "ALPHA = 2.0\n",
    "BETA = 0.5\n",
    "TAU = 1e-3\n",
    "LAMBDA_GRQO = 1.0\n",
    "TEACHER_EMA = 0.99\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "# --- 3) GRQO Decoder ---\n",
    "grqo_model = GRQO(\n",
    "    Hidden_dim=HIDDEN_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    num_tokens=NUM_TOKENS,\n",
    "    ddropout=DDROPOUT,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    temperature=TEMPERATURE,\n",
    "    alpha=ALPHA,\n",
    "    beta=BETA,\n",
    "    tau=TAU,\n",
    "    lambda_grqo=LAMBDA_GRQO,\n",
    "    teacher_ema=TEACHER_EMA,\n",
    "    reward_proxy=\"taylor\"\n",
    ")\n",
    "\n",
    "# --- 4) Full ViTGRQO Model ---\n",
    "class ViTGRQO(nn.Module):\n",
    "    def __init__(self, vit_encoder, grqo_model):\n",
    "        super().__init__()\n",
    "        self.vit = vit_encoder\n",
    "        self.grqo = grqo_model\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        # Get patch embeddings from HF ViT (exclude CLS token)\n",
    "        outputs = self.vit(pixel_values=x, output_hidden_states=True)\n",
    "        patch_tokens = outputs.last_hidden_state[:, 1:, :]  # [B, N, D] skip CLS token\n",
    "        \n",
    "        # Pass to GRQO\n",
    "        if labels is not None:\n",
    "            return self.grqo(patch_tokens, labels)\n",
    "        else:\n",
    "            dummy_labels = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "            grqo_out = self.grqo(patch_tokens, dummy_labels)\n",
    "            return {\n",
    "                'img_logits': grqo_out['img_logits'],\n",
    "                'preds': grqo_out['preds']\n",
    "            }\n",
    "\n",
    "# --- 5) Initialize model ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ViTGRQO(vit_encoder, grqo_model).to(device)\n",
    "\n",
    "# --- 6) Freeze ViT backbone if desired ---\n",
    "FREEZE_VIT = False\n",
    "if FREEZE_VIT:\n",
    "    for param in model.vit.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"ViT backbone frozen\")\n",
    "else:\n",
    "    print(\"ViT backbone trainable\")\n",
    "\n",
    "# Ensure GRQO is trainable\n",
    "for param in model.grqo.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# --- 7) Optimizer ---\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=LR, weight_decay=0.01)\n",
    "\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Hidden dim: {HIDDEN_DIM}, Num tokens: {NUM_TOKENS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7e896",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da2f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_grqo_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(images, labels)\n",
    "        \n",
    "        # Extract losses\n",
    "        loss = output['loss']\n",
    "        cls_loss = output['cls_loss']\n",
    "        grqo_loss = output['grqo_loss']\n",
    "        preds = output['preds']\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        total_cls_loss += cls_loss.item() * images.size(0)\n",
    "        total_grqo_loss += grqo_loss.item() * images.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}, '\n",
    "                  f'Cls: {cls_loss.item():.4f}, GRQO: {grqo_loss.item():.4f}')\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_cls_loss = total_cls_loss / total_samples\n",
    "    avg_grqo_loss = total_grqo_loss / total_samples\n",
    "    accuracy = correct / total_samples\n",
    "    \n",
    "    return avg_loss, avg_cls_loss, avg_grqo_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dbc43",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c48930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # GRQO needs gradients even during validation\n",
    "        with torch.set_grad_enabled(True):\n",
    "            output = model(images, labels)\n",
    "            preds = output['preds']\n",
    "        \n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3fb6ed",
   "metadata": {},
   "source": [
    "### finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a010c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LODO: Leaving out domain 'sketch' ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 1.9996, Cls: 1.9305, GRQO: 0.0691\n",
      "Batch 50, Loss: 1.0056, Cls: 0.9952, GRQO: 0.0103\n",
      "Batch 100, Loss: 0.5724, Cls: 0.5644, GRQO: 0.0080\n",
      "Batch 150, Loss: 0.4117, Cls: 0.4049, GRQO: 0.0067\n",
      "Train - Loss: 0.9679, Cls: 0.9554, GRQO: 0.0126, Acc: 0.7237\n",
      "Val Acc (sketch): 0.3550\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.6012, Cls: 0.5945, GRQO: 0.0067\n",
      "Batch 50, Loss: 0.2280, Cls: 0.2226, GRQO: 0.0053\n",
      "Batch 100, Loss: 0.2176, Cls: 0.2124, GRQO: 0.0051\n",
      "Batch 150, Loss: 0.1684, Cls: 0.1635, GRQO: 0.0049\n",
      "Train - Loss: 0.2815, Cls: 0.2760, GRQO: 0.0054, Acc: 0.9130\n",
      "Val Acc (sketch): 0.4084\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.0988, Cls: 0.0939, GRQO: 0.0050\n",
      "Batch 50, Loss: 0.1454, Cls: 0.1411, GRQO: 0.0043\n",
      "Batch 100, Loss: 0.2461, Cls: 0.2415, GRQO: 0.0045\n",
      "Batch 150, Loss: 0.0480, Cls: 0.0441, GRQO: 0.0039\n",
      "Train - Loss: 0.1665, Cls: 0.1623, GRQO: 0.0042, Acc: 0.9511\n",
      "Val Acc (sketch): 0.3893\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0905, Cls: 0.0866, GRQO: 0.0040\n",
      "Batch 50, Loss: 0.1168, Cls: 0.1126, GRQO: 0.0042\n",
      "Batch 100, Loss: 0.0562, Cls: 0.0522, GRQO: 0.0040\n",
      "Batch 150, Loss: 0.1223, Cls: 0.1187, GRQO: 0.0037\n",
      "Train - Loss: 0.0977, Cls: 0.0939, GRQO: 0.0038, Acc: 0.9769\n",
      "Val Acc (sketch): 0.4008\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.1230, Cls: 0.1195, GRQO: 0.0035\n",
      "Batch 50, Loss: 0.0824, Cls: 0.0787, GRQO: 0.0037\n",
      "Batch 100, Loss: 0.0238, Cls: 0.0206, GRQO: 0.0032\n",
      "Batch 150, Loss: 0.0710, Cls: 0.0678, GRQO: 0.0033\n",
      "Train - Loss: 0.0551, Cls: 0.0516, GRQO: 0.0035, Acc: 0.9903\n",
      "Val Acc (sketch): 0.4020\n",
      "Best Val Acc for sketch: 0.4084\n",
      "\n",
      "=== LODO: Leaving out domain 'photo' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 1.5363, Cls: 1.5329, GRQO: 0.0034\n",
      "Batch 50, Loss: 0.5699, Cls: 0.5668, GRQO: 0.0031\n",
      "Batch 100, Loss: 0.7308, Cls: 0.7277, GRQO: 0.0032\n",
      "Batch 150, Loss: 0.4507, Cls: 0.4477, GRQO: 0.0031\n",
      "Batch 200, Loss: 0.4191, Cls: 0.4161, GRQO: 0.0030\n",
      "Train - Loss: 0.4736, Cls: 0.4705, GRQO: 0.0032, Acc: 0.8326\n",
      "Val Acc (photo): 0.9760\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.2050, Cls: 0.2024, GRQO: 0.0026\n",
      "Batch 50, Loss: 0.3292, Cls: 0.3264, GRQO: 0.0028\n",
      "Batch 100, Loss: 0.1572, Cls: 0.1541, GRQO: 0.0031\n",
      "Batch 150, Loss: 0.2946, Cls: 0.2916, GRQO: 0.0030\n",
      "Batch 200, Loss: 0.2474, Cls: 0.2445, GRQO: 0.0028\n",
      "Train - Loss: 0.2927, Cls: 0.2898, GRQO: 0.0029, Acc: 0.8981\n",
      "Val Acc (photo): 0.9760\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.2360, Cls: 0.2332, GRQO: 0.0028\n",
      "Batch 50, Loss: 0.3043, Cls: 0.3014, GRQO: 0.0030\n",
      "Batch 100, Loss: 0.1305, Cls: 0.1274, GRQO: 0.0030\n",
      "Batch 150, Loss: 0.1644, Cls: 0.1614, GRQO: 0.0030\n",
      "Batch 200, Loss: 0.2298, Cls: 0.2271, GRQO: 0.0027\n",
      "Train - Loss: 0.2288, Cls: 0.2260, GRQO: 0.0029, Acc: 0.9184\n",
      "Val Acc (photo): 0.9820\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.2114, Cls: 0.2088, GRQO: 0.0026\n",
      "Batch 50, Loss: 0.0600, Cls: 0.0571, GRQO: 0.0029\n",
      "Batch 100, Loss: 0.1976, Cls: 0.1947, GRQO: 0.0029\n",
      "Batch 150, Loss: 0.2332, Cls: 0.2304, GRQO: 0.0027\n",
      "Batch 200, Loss: 0.1838, Cls: 0.1808, GRQO: 0.0030\n",
      "Train - Loss: 0.1753, Cls: 0.1724, GRQO: 0.0028, Acc: 0.9381\n",
      "Val Acc (photo): 0.9760\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.1018, Cls: 0.0991, GRQO: 0.0026\n",
      "Batch 50, Loss: 0.1247, Cls: 0.1219, GRQO: 0.0028\n",
      "Batch 100, Loss: 0.1314, Cls: 0.1285, GRQO: 0.0028\n",
      "Batch 150, Loss: 0.0835, Cls: 0.0807, GRQO: 0.0028\n",
      "Batch 200, Loss: 0.0929, Cls: 0.0901, GRQO: 0.0028\n",
      "Train - Loss: 0.1354, Cls: 0.1326, GRQO: 0.0028, Acc: 0.9563\n",
      "Val Acc (photo): 0.9701\n",
      "Best Val Acc for photo: 0.9820\n",
      "\n",
      "=== LODO: Leaving out domain 'art_painting' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 0.0822, Cls: 0.0796, GRQO: 0.0026\n",
      "Batch 50, Loss: 0.1612, Cls: 0.1585, GRQO: 0.0027\n",
      "Batch 100, Loss: 0.2492, Cls: 0.2465, GRQO: 0.0027\n",
      "Batch 150, Loss: 0.1435, Cls: 0.1408, GRQO: 0.0027\n",
      "Train - Loss: 0.1033, Cls: 0.1006, GRQO: 0.0028, Acc: 0.9681\n",
      "Val Acc (art_painting): 0.8756\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.1157, Cls: 0.1128, GRQO: 0.0028\n",
      "Batch 50, Loss: 0.0980, Cls: 0.0953, GRQO: 0.0028\n",
      "Batch 100, Loss: 0.0851, Cls: 0.0825, GRQO: 0.0026\n",
      "Batch 150, Loss: 0.0371, Cls: 0.0346, GRQO: 0.0025\n",
      "Train - Loss: 0.0621, Cls: 0.0593, GRQO: 0.0028, Acc: 0.9810\n",
      "Val Acc (art_painting): 0.8756\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.0453, Cls: 0.0429, GRQO: 0.0025\n",
      "Batch 50, Loss: 0.0222, Cls: 0.0195, GRQO: 0.0026\n",
      "Batch 100, Loss: 0.0250, Cls: 0.0224, GRQO: 0.0026\n",
      "Batch 150, Loss: 0.0572, Cls: 0.0546, GRQO: 0.0026\n",
      "Train - Loss: 0.0406, Cls: 0.0380, GRQO: 0.0027, Acc: 0.9898\n",
      "Val Acc (art_painting): 0.8683\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0065, Cls: 0.0037, GRQO: 0.0028\n",
      "Batch 50, Loss: 0.0699, Cls: 0.0673, GRQO: 0.0027\n",
      "Batch 100, Loss: 0.0057, Cls: 0.0032, GRQO: 0.0025\n",
      "Batch 150, Loss: 0.0178, Cls: 0.0152, GRQO: 0.0025\n",
      "Train - Loss: 0.0298, Cls: 0.0271, GRQO: 0.0026, Acc: 0.9921\n",
      "Val Acc (art_painting): 0.8707\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0272, Cls: 0.0247, GRQO: 0.0025\n",
      "Batch 50, Loss: 0.0140, Cls: 0.0115, GRQO: 0.0025\n",
      "Batch 100, Loss: 0.0072, Cls: 0.0048, GRQO: 0.0024\n",
      "Batch 150, Loss: 0.0130, Cls: 0.0107, GRQO: 0.0023\n",
      "Train - Loss: 0.0120, Cls: 0.0096, GRQO: 0.0024, Acc: 0.9995\n",
      "Val Acc (art_painting): 0.8756\n",
      "Best Val Acc for art_painting: 0.8756\n",
      "\n",
      "=== LODO: Leaving out domain 'cartoon' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 0.0108, Cls: 0.0085, GRQO: 0.0023\n",
      "Batch 50, Loss: 0.0137, Cls: 0.0115, GRQO: 0.0022\n",
      "Batch 100, Loss: 0.0135, Cls: 0.0111, GRQO: 0.0024\n",
      "Batch 150, Loss: 0.0235, Cls: 0.0212, GRQO: 0.0022\n",
      "Train - Loss: 0.0281, Cls: 0.0258, GRQO: 0.0022, Acc: 0.9920\n",
      "Val Acc (cartoon): 0.9041\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.0043, Cls: 0.0023, GRQO: 0.0020\n",
      "Batch 50, Loss: 0.0294, Cls: 0.0272, GRQO: 0.0022\n",
      "Batch 100, Loss: 0.0080, Cls: 0.0058, GRQO: 0.0022\n",
      "Batch 150, Loss: 0.0114, Cls: 0.0093, GRQO: 0.0020\n",
      "Train - Loss: 0.0185, Cls: 0.0163, GRQO: 0.0021, Acc: 0.9964\n",
      "Val Acc (cartoon): 0.8998\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.0067, Cls: 0.0047, GRQO: 0.0020\n",
      "Batch 50, Loss: 0.0051, Cls: 0.0032, GRQO: 0.0019\n",
      "Batch 100, Loss: 0.0044, Cls: 0.0025, GRQO: 0.0019\n",
      "Batch 150, Loss: 0.0097, Cls: 0.0079, GRQO: 0.0017\n",
      "Train - Loss: 0.0143, Cls: 0.0123, GRQO: 0.0020, Acc: 0.9971\n",
      "Val Acc (cartoon): 0.8955\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0394, Cls: 0.0376, GRQO: 0.0018\n",
      "Batch 50, Loss: 0.0048, Cls: 0.0029, GRQO: 0.0019\n",
      "Batch 100, Loss: 0.0023, Cls: 0.0004, GRQO: 0.0019\n",
      "Batch 150, Loss: 0.0065, Cls: 0.0046, GRQO: 0.0019\n",
      "Train - Loss: 0.0216, Cls: 0.0197, GRQO: 0.0019, Acc: 0.9940\n",
      "Val Acc (cartoon): 0.8742\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0392, Cls: 0.0373, GRQO: 0.0018\n",
      "Batch 50, Loss: 0.0308, Cls: 0.0290, GRQO: 0.0018\n",
      "Batch 100, Loss: 0.0105, Cls: 0.0088, GRQO: 0.0017\n",
      "Batch 150, Loss: 0.0058, Cls: 0.0042, GRQO: 0.0016\n",
      "Train - Loss: 0.0128, Cls: 0.0111, GRQO: 0.0018, Acc: 0.9967\n",
      "Val Acc (cartoon): 0.8913\n",
      "Best Val Acc for cartoon: 0.9041\n",
      "\n",
      "==================================================\n",
      "LODO RESULTS SUMMARY\n",
      "==================================================\n",
      "sketch         : 0.4084\n",
      "photo          : 0.9820\n",
      "art_painting   : 0.8756\n",
      "cartoon        : 0.9041\n",
      "Average        : 0.7925\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "ALL_DOMAINS = ['sketch', 'photo', 'art_painting', 'cartoon']\n",
    "lodo_results = {}\n",
    "\n",
    "for LEAVE_OUT in ALL_DOMAINS:\n",
    "    print(f\"\\n=== LODO: Leaving out domain '{LEAVE_OUT}' ===\")\n",
    "    TRAIN_DOMAINS = [d for d in ALL_DOMAINS if d != LEAVE_OUT]\n",
    "    VAL_DOMAIN = LEAVE_OUT\n",
    "\n",
    "    # Create data loaders\n",
    "    train_datasets = [pacs_data.get_dataloader(domain=d, train=True) for d in TRAIN_DOMAINS]\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.ConcatDataset([d.dataset for d in train_datasets]),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_loader = pacs_data.get_dataloader(domain=VAL_DOMAIN, train=False)\n",
    "\n",
    "    # Reset model for this split\n",
    "    model = ViTGRQO(vit_encoder, grqo_model).to(DEVICE)\n",
    "    \n",
    "    if FREEZE_VIT:\n",
    "        for param in model.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=LR, weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_cls, train_grqo, train_acc = train_epoch(model, train_loader, optimizer, DEVICE)\n",
    "        \n",
    "        # Validate  \n",
    "        val_acc = validate(model, val_loader, DEVICE)\n",
    "        \n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Cls: {train_cls:.4f}, \"\n",
    "              f\"GRQO: {train_grqo:.4f}, Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Acc ({VAL_DOMAIN}): {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    lodo_results[VAL_DOMAIN] = best_val_acc\n",
    "    print(f\"Best Val Acc for {VAL_DOMAIN}: {best_val_acc:.4f}\")\n",
    "\n",
    "# Cell 8: Results Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LODO RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for domain, acc in lodo_results.items():\n",
    "    print(f\"{domain:15}: {acc:.4f}\")\n",
    "\n",
    "avg_lodo = sum(lodo_results.values()) / len(lodo_results)\n",
    "print(f\"{'Average':15}: {avg_lodo:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913f84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LODO: Leaving out domain 'sketch' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 2.0397, Cls: 1.9501, GRQO: 0.0896\n",
      "Batch 50, Loss: 0.4309, Cls: 0.4234, GRQO: 0.0075\n",
      "Batch 100, Loss: 0.4242, Cls: 0.4194, GRQO: 0.0047\n",
      "Batch 150, Loss: 0.3062, Cls: 0.3018, GRQO: 0.0044\n",
      "Train - Loss: 0.6380, Cls: 0.6268, GRQO: 0.0111, Acc: 0.8051\n",
      "Val Acc (sketch): 0.5153\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.2020, Cls: 0.1980, GRQO: 0.0040\n",
      "Batch 50, Loss: 0.0992, Cls: 0.0957, GRQO: 0.0035\n",
      "Batch 100, Loss: 0.0685, Cls: 0.0654, GRQO: 0.0031\n",
      "Batch 150, Loss: 0.0799, Cls: 0.0770, GRQO: 0.0029\n",
      "Train - Loss: 0.1189, Cls: 0.1154, GRQO: 0.0035, Acc: 0.9660\n",
      "Val Acc (sketch): 0.4962\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.1174, Cls: 0.1143, GRQO: 0.0031\n",
      "Batch 50, Loss: 0.0283, Cls: 0.0257, GRQO: 0.0027\n",
      "Batch 100, Loss: 0.0125, Cls: 0.0100, GRQO: 0.0025\n",
      "Batch 150, Loss: 0.0126, Cls: 0.0102, GRQO: 0.0023\n",
      "Train - Loss: 0.0517, Cls: 0.0490, GRQO: 0.0026, Acc: 0.9854\n",
      "Val Acc (sketch): 0.6718\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0226, Cls: 0.0201, GRQO: 0.0024\n",
      "Batch 50, Loss: 0.0626, Cls: 0.0605, GRQO: 0.0021\n",
      "Batch 100, Loss: 0.0164, Cls: 0.0144, GRQO: 0.0020\n",
      "Batch 150, Loss: 0.0099, Cls: 0.0080, GRQO: 0.0019\n",
      "Train - Loss: 0.0358, Cls: 0.0336, GRQO: 0.0022, Acc: 0.9907\n",
      "Val Acc (sketch): 0.4746\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0148, Cls: 0.0129, GRQO: 0.0020\n",
      "Batch 50, Loss: 0.0206, Cls: 0.0187, GRQO: 0.0019\n",
      "Batch 100, Loss: 0.0337, Cls: 0.0317, GRQO: 0.0019\n",
      "Batch 150, Loss: 0.0100, Cls: 0.0082, GRQO: 0.0018\n",
      "Train - Loss: 0.0160, Cls: 0.0141, GRQO: 0.0019, Acc: 0.9973\n",
      "Val Acc (sketch): 0.5382\n",
      "Best Val Acc for sketch: 0.6718\n",
      "\n",
      "=== LODO: Leaving out domain 'photo' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 0.7912, Cls: 0.7891, GRQO: 0.0020\n",
      "Batch 50, Loss: 0.2252, Cls: 0.2233, GRQO: 0.0019\n",
      "Batch 100, Loss: 0.1199, Cls: 0.1183, GRQO: 0.0016\n",
      "Batch 150, Loss: 0.2575, Cls: 0.2560, GRQO: 0.0015\n",
      "Batch 200, Loss: 0.1489, Cls: 0.1473, GRQO: 0.0017\n",
      "Train - Loss: 0.3082, Cls: 0.3064, GRQO: 0.0018, Acc: 0.8954\n",
      "Val Acc (photo): 0.9671\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.0580, Cls: 0.0563, GRQO: 0.0017\n",
      "Batch 50, Loss: 0.0990, Cls: 0.0974, GRQO: 0.0016\n",
      "Batch 100, Loss: 0.4229, Cls: 0.4215, GRQO: 0.0015\n",
      "Batch 150, Loss: 0.2244, Cls: 0.2230, GRQO: 0.0014\n",
      "Batch 200, Loss: 0.0620, Cls: 0.0607, GRQO: 0.0014\n",
      "Train - Loss: 0.1474, Cls: 0.1459, GRQO: 0.0015, Acc: 0.9494\n",
      "Val Acc (photo): 0.9701\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.0876, Cls: 0.0862, GRQO: 0.0014\n",
      "Batch 50, Loss: 0.1444, Cls: 0.1430, GRQO: 0.0014\n",
      "Batch 100, Loss: 0.0160, Cls: 0.0148, GRQO: 0.0012\n",
      "Batch 150, Loss: 0.0363, Cls: 0.0351, GRQO: 0.0012\n",
      "Batch 200, Loss: 0.0102, Cls: 0.0091, GRQO: 0.0011\n",
      "Train - Loss: 0.0808, Cls: 0.0796, GRQO: 0.0013, Acc: 0.9733\n",
      "Val Acc (photo): 0.9611\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0690, Cls: 0.0679, GRQO: 0.0012\n",
      "Batch 50, Loss: 0.0081, Cls: 0.0071, GRQO: 0.0010\n",
      "Batch 100, Loss: 0.0275, Cls: 0.0263, GRQO: 0.0011\n",
      "Batch 150, Loss: 0.0611, Cls: 0.0601, GRQO: 0.0011\n",
      "Batch 200, Loss: 0.1370, Cls: 0.1360, GRQO: 0.0010\n",
      "Train - Loss: 0.0636, Cls: 0.0625, GRQO: 0.0011, Acc: 0.9785\n",
      "Val Acc (photo): 0.9731\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0569, Cls: 0.0559, GRQO: 0.0010\n",
      "Batch 50, Loss: 0.0199, Cls: 0.0189, GRQO: 0.0010\n",
      "Batch 100, Loss: 0.0033, Cls: 0.0025, GRQO: 0.0009\n",
      "Batch 150, Loss: 0.1298, Cls: 0.1290, GRQO: 0.0008\n",
      "Batch 200, Loss: 0.0128, Cls: 0.0120, GRQO: 0.0008\n",
      "Train - Loss: 0.0712, Cls: 0.0702, GRQO: 0.0009, Acc: 0.9760\n",
      "Val Acc (photo): 0.9521\n",
      "Best Val Acc for photo: 0.9731\n",
      "\n",
      "=== LODO: Leaving out domain 'art_painting' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 0.0591, Cls: 0.0582, GRQO: 0.0009\n",
      "Batch 50, Loss: 0.0809, Cls: 0.0801, GRQO: 0.0008\n",
      "Batch 100, Loss: 0.0398, Cls: 0.0391, GRQO: 0.0007\n",
      "Batch 150, Loss: 0.0623, Cls: 0.0617, GRQO: 0.0007\n",
      "Train - Loss: 0.0732, Cls: 0.0725, GRQO: 0.0008, Acc: 0.9764\n",
      "Val Acc (art_painting): 0.8976\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.0050, Cls: 0.0043, GRQO: 0.0007\n",
      "Batch 50, Loss: 0.0682, Cls: 0.0676, GRQO: 0.0006\n",
      "Batch 100, Loss: 0.0042, Cls: 0.0036, GRQO: 0.0006\n",
      "Batch 150, Loss: 0.0203, Cls: 0.0198, GRQO: 0.0006\n",
      "Train - Loss: 0.0399, Cls: 0.0393, GRQO: 0.0006, Acc: 0.9858\n",
      "Val Acc (art_painting): 0.8732\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.0091, Cls: 0.0085, GRQO: 0.0006\n",
      "Batch 50, Loss: 0.0509, Cls: 0.0504, GRQO: 0.0006\n",
      "Batch 100, Loss: 0.0046, Cls: 0.0041, GRQO: 0.0005\n",
      "Batch 150, Loss: 0.0052, Cls: 0.0047, GRQO: 0.0005\n",
      "Train - Loss: 0.0565, Cls: 0.0559, GRQO: 0.0005, Acc: 0.9832\n",
      "Val Acc (art_painting): 0.8488\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0044, Cls: 0.0039, GRQO: 0.0005\n",
      "Batch 50, Loss: 0.0264, Cls: 0.0260, GRQO: 0.0004\n",
      "Batch 100, Loss: 0.0287, Cls: 0.0283, GRQO: 0.0004\n",
      "Batch 150, Loss: 0.3691, Cls: 0.3687, GRQO: 0.0004\n",
      "Train - Loss: 0.0579, Cls: 0.0575, GRQO: 0.0004, Acc: 0.9800\n",
      "Val Acc (art_painting): 0.8244\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 0.0136, Cls: 0.0133, GRQO: 0.0004\n",
      "Batch 50, Loss: 0.1400, Cls: 0.1397, GRQO: 0.0004\n",
      "Batch 100, Loss: 0.0058, Cls: 0.0055, GRQO: 0.0003\n",
      "Batch 150, Loss: 0.0036, Cls: 0.0033, GRQO: 0.0004\n",
      "Train - Loss: 0.0328, Cls: 0.0325, GRQO: 0.0003, Acc: 0.9898\n",
      "Val Acc (art_painting): 0.8537\n",
      "Best Val Acc for art_painting: 0.8976\n",
      "\n",
      "=== LODO: Leaving out domain 'cartoon' ===\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 0.2034, Cls: 0.2031, GRQO: 0.0003\n",
      "Batch 50, Loss: 0.0052, Cls: 0.0049, GRQO: 0.0003\n",
      "Batch 100, Loss: 0.2164, Cls: 0.2161, GRQO: 0.0003\n",
      "Batch 150, Loss: 0.0123, Cls: 0.0120, GRQO: 0.0003\n",
      "Train - Loss: 0.0713, Cls: 0.0710, GRQO: 0.0003, Acc: 0.9773\n",
      "Val Acc (cartoon): 0.9062\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 0.0107, Cls: 0.0104, GRQO: 0.0003\n",
      "Batch 50, Loss: 0.0495, Cls: 0.0492, GRQO: 0.0003\n",
      "Batch 100, Loss: 0.0060, Cls: 0.0057, GRQO: 0.0003\n",
      "Batch 150, Loss: 0.0022, Cls: 0.0020, GRQO: 0.0002\n",
      "Train - Loss: 0.0399, Cls: 0.0397, GRQO: 0.0003, Acc: 0.9881\n",
      "Val Acc (cartoon): 0.9168\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 0.0045, Cls: 0.0043, GRQO: 0.0002\n",
      "Batch 50, Loss: 0.0022, Cls: 0.0020, GRQO: 0.0002\n",
      "Batch 100, Loss: 0.0024, Cls: 0.0021, GRQO: 0.0002\n",
      "Batch 150, Loss: 0.0022, Cls: 0.0020, GRQO: 0.0002\n",
      "Train - Loss: 0.0275, Cls: 0.0273, GRQO: 0.0002, Acc: 0.9920\n",
      "Val Acc (cartoon): 0.9190\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 0.0016, Cls: 0.0014, GRQO: 0.0002\n",
      "Batch 50, Loss: 0.0338, Cls: 0.0336, GRQO: 0.0002\n",
      "Batch 100, Loss: 0.1924, Cls: 0.1922, GRQO: 0.0002\n"
     ]
    }
   ],
   "source": [
    "ALL_DOMAINS = ['sketch', 'photo', 'art_painting', 'cartoon']\n",
    "lodo_results = {}\n",
    "\n",
    "for LEAVE_OUT in ALL_DOMAINS:\n",
    "    print(f\"\\n=== LODO: Leaving out domain '{LEAVE_OUT}' ===\")\n",
    "    TRAIN_DOMAINS = [d for d in ALL_DOMAINS if d != LEAVE_OUT]\n",
    "    VAL_DOMAIN = LEAVE_OUT\n",
    "\n",
    "    # Create data loaders\n",
    "    train_datasets = [pacs_data.get_dataloader(domain=d, train=True) for d in TRAIN_DOMAINS]\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.ConcatDataset([d.dataset for d in train_datasets]),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_loader = pacs_data.get_dataloader(domain=VAL_DOMAIN, train=False)\n",
    "\n",
    "    # Reset model for this split\n",
    "    model = ViTGRQO(vit_encoder, grqo_model).to(DEVICE)\n",
    "    \n",
    "    if FREEZE_VIT:\n",
    "        for param in model.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=LR, weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_cls, train_grqo, train_acc = train_epoch(model, train_loader, optimizer, DEVICE)\n",
    "        \n",
    "        # Validate  \n",
    "        val_acc = validate(model, val_loader, DEVICE)\n",
    "        \n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Cls: {train_cls:.4f}, \"\n",
    "              f\"GRQO: {train_grqo:.4f}, Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Acc ({VAL_DOMAIN}): {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    lodo_results[VAL_DOMAIN] = best_val_acc\n",
    "    print(f\"Best Val Acc for {VAL_DOMAIN}: {best_val_acc:.4f}\")\n",
    "\n",
    "# Cell 8: Results Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LODO RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for domain, acc in lodo_results.items():\n",
    "    print(f\"{domain:15}: {acc:.4f}\")\n",
    "\n",
    "avg_lodo = sum(lodo_results.values()) / len(lodo_results)\n",
    "print(f\"{'Average':15}: {avg_lodo:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6231c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LODO: Leaving out domain 'sketch' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([7, 192]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Train - Loss: 0.3371, Acc: 0.8824\n",
      "Val Acc (sketch): 0.5522\n",
      "\n",
      "Epoch 2/5\n",
      "Train - Loss: 0.0538, Acc: 0.9843\n",
      "Val Acc (sketch): 0.4326\n",
      "\n",
      "Epoch 3/5\n",
      "Train - Loss: 0.0298, Acc: 0.9913\n",
      "Val Acc (sketch): 0.5267\n",
      "\n",
      "Epoch 4/5\n",
      "Train - Loss: 0.0272, Acc: 0.9911\n",
      "Val Acc (sketch): 0.5674\n",
      "\n",
      "Epoch 5/5\n",
      "Train - Loss: 0.0189, Acc: 0.9934\n",
      "Val Acc (sketch): 0.4898\n",
      "Best Val Acc for sketch: 0.5674\n",
      "\n",
      "=== LODO: Leaving out domain 'photo' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([7, 192]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Train - Loss: 0.4923, Acc: 0.8202\n",
      "Val Acc (photo): 0.9671\n",
      "\n",
      "Epoch 2/5\n",
      "Train - Loss: 0.1340, Acc: 0.9530\n",
      "Val Acc (photo): 0.9701\n",
      "\n",
      "Epoch 3/5\n",
      "Train - Loss: 0.0638, Acc: 0.9794\n",
      "Val Acc (photo): 0.9611\n",
      "\n",
      "Epoch 4/5\n",
      "Train - Loss: 0.0537, Acc: 0.9805\n",
      "Val Acc (photo): 0.9641\n",
      "\n",
      "Epoch 5/5\n",
      "Train - Loss: 0.0392, Acc: 0.9859\n",
      "Val Acc (photo): 0.9641\n",
      "Best Val Acc for photo: 0.9701\n",
      "\n",
      "=== LODO: Leaving out domain 'art_painting' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([7, 192]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Train - Loss: 0.4586, Acc: 0.8336\n",
      "Val Acc (art_painting): 0.7902\n",
      "\n",
      "Epoch 2/5\n",
      "Train - Loss: 0.1261, Acc: 0.9569\n",
      "Val Acc (art_painting): 0.7683\n",
      "\n",
      "Epoch 3/5\n",
      "Train - Loss: 0.0563, Acc: 0.9781\n",
      "Val Acc (art_painting): 0.7098\n",
      "\n",
      "Epoch 4/5\n",
      "Train - Loss: 0.0543, Acc: 0.9806\n",
      "Val Acc (art_painting): 0.6927\n",
      "\n",
      "Epoch 5/5\n",
      "Train - Loss: 0.0507, Acc: 0.9832\n",
      "Val Acc (art_painting): 0.7707\n",
      "Best Val Acc for art_painting: 0.7902\n",
      "\n",
      "=== LODO: Leaving out domain 'cartoon' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([7, 192]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Train - Loss: 0.5261, Acc: 0.8060\n",
      "Val Acc (cartoon): 0.6930\n",
      "\n",
      "Epoch 2/5\n",
      "Train - Loss: 0.1368, Acc: 0.9534\n",
      "Val Acc (cartoon): 0.7783\n",
      "\n",
      "Epoch 3/5\n",
      "Train - Loss: 0.0692, Acc: 0.9761\n",
      "Val Acc (cartoon): 0.7420\n",
      "\n",
      "Epoch 4/5\n",
      "Train - Loss: 0.0493, Acc: 0.9827\n",
      "Val Acc (cartoon): 0.7399\n",
      "\n",
      "Epoch 5/5\n",
      "Train - Loss: 0.0551, Acc: 0.9822\n",
      "Val Acc (cartoon): 0.7058\n",
      "Best Val Acc for cartoon: 0.7783\n",
      "\n",
      "=== Baseline: Train & Validate on All Domains ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([7, 192]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'baseline_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 112\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Model (no freezing)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m model \u001b[38;5;241m=\u001b[39m ViTForImageClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWinKawaks/vit-tiny-patch16-224\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39mNUM_CLASSES,\n\u001b[1;32m    109\u001b[0m     ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \n\u001b[1;32m    110\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 112\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\u001b[43mbaseline_model\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m    113\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    115\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline_model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "ALL_DOMAINS = ['sketch', 'photo', 'art_painting', 'cartoon']\n",
    "lodo_results = {}\n",
    "\n",
    "# ---------------- LODO Experiments ----------------\n",
    "for LEAVE_OUT in ALL_DOMAINS:\n",
    "    print(f\"\\n=== LODO: Leaving out domain '{LEAVE_OUT}' ===\")\n",
    "    TRAIN_DOMAINS = [d for d in ALL_DOMAINS if d != LEAVE_OUT]\n",
    "    VAL_DOMAIN = LEAVE_OUT\n",
    "\n",
    "    # Train loader\n",
    "    train_datasets = [pacs_data.get_dataloader(domain=d, train=True) for d in TRAIN_DOMAINS]\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.ConcatDataset([d.dataset for d in train_datasets]),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Validation loader\n",
    "    val_loader = pacs_data.get_dataloader(domain=VAL_DOMAIN, train=False)\n",
    "\n",
    "    # Model (no freezing)\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        \"WinKawaks/vit-tiny-patch16-224\",\n",
    "        num_labels=NUM_CLASSES,\n",
    "        ignore_mismatched_sizes=True  # <--- important fix\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        running_loss, running_corrects, running_samples = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            running_samples += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / running_samples\n",
    "        train_acc = running_corrects / running_samples\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.logits.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_acc = correct / total\n",
    "\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Acc ({VAL_DOMAIN}): {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    lodo_results[VAL_DOMAIN] = best_val_acc\n",
    "    print(f\"Best Val Acc for {VAL_DOMAIN}: {best_val_acc:.4f}\")\n",
    "\n",
    "\n",
    "# ---------------- All-Domains Baseline ----------------\n",
    "print(\"\\n=== Baseline: Train & Validate on All Domains ===\")\n",
    "\n",
    "# Train loader with all domains\n",
    "all_train_datasets = [pacs_data.get_dataloader(domain=d, train=True) for d in ALL_DOMAINS]\n",
    "train_loader_all = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.ConcatDataset([d.dataset for d in all_train_datasets]),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation loader with all domains\n",
    "all_val_datasets = [pacs_data.get_dataloader(domain=d, train=False) for d in ALL_DOMAINS]\n",
    "val_loader_all = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.ConcatDataset([d.dataset for d in all_val_datasets]),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Model (no freezing)\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"WinKawaks/vit-tiny-patch16-224\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True \n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(baseline_model.parameters(), lr=LR, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    # ---- Train ----\n",
    "    baseline_model.train()\n",
    "    running_loss, running_corrects, running_samples = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in train_loader_all:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = baseline_model(images)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += (preds == labels).sum().item()\n",
    "        running_samples += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / running_samples\n",
    "    train_acc = running_corrects / running_samples\n",
    "\n",
    "    # ---- Validation ----\n",
    "    baseline_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_all:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = baseline_model(images)\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Acc (All Domains): {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "\n",
    "\n",
    "# ---------------- Results Summary ----------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LODO RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for domain, acc in lodo_results.items():\n",
    "    print(f\"{domain:15}: {acc:.4f}\")\n",
    "\n",
    "avg_lodo = sum(lodo_results.values()) / len(lodo_results)\n",
    "print(f\"{'Average LODO':15}: {avg_lodo:.4f}\")\n",
    "print(f\"{'All-domains baseline':15}: {best_val_acc:.4f}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1676ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline: Train & Validate on All Domains ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([7, 192]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Train - Loss: 0.4178, Acc: 0.8486\n",
      "Val Acc (All Domains Combined): 0.9145\n",
      "  sketch         : 0.8766\n",
      "  photo          : 0.9820\n",
      "  art_painting   : 0.9268\n",
      "  cartoon        : 0.9190\n",
      "\n",
      "Epoch 2/5\n",
      "Train - Loss: 0.1145, Acc: 0.9592\n",
      "Val Acc (All Domains Combined): 0.9050\n",
      "  sketch         : 0.8448\n",
      "  photo          : 0.9820\n",
      "  art_painting   : 0.9220\n",
      "  cartoon        : 0.9360\n",
      "\n",
      "Epoch 3/5\n",
      "Train - Loss: 0.0513, Acc: 0.9810\n",
      "Val Acc (All Domains Combined): 0.9195\n",
      "  sketch         : 0.8995\n",
      "  photo          : 0.9611\n",
      "  art_painting   : 0.9024\n",
      "  cartoon        : 0.9382\n",
      "\n",
      "Epoch 4/5\n",
      "Train - Loss: 0.0517, Acc: 0.9835\n",
      "Val Acc (All Domains Combined): 0.9190\n",
      "  sketch         : 0.8893\n",
      "  photo          : 0.9820\n",
      "  art_painting   : 0.8951\n",
      "  cartoon        : 0.9446\n",
      "\n",
      "Epoch 5/5\n",
      "Train - Loss: 0.0416, Acc: 0.9862\n",
      "Val Acc (All Domains Combined): 0.9190\n",
      "  sketch         : 0.9148\n",
      "  photo          : 0.9581\n",
      "  art_painting   : 0.8585\n",
      "  cartoon        : 0.9510\n",
      "\n",
      "==================================================\n",
      "LODO RESULTS SUMMARY\n",
      "==================================================\n",
      "sketch         : 0.5674\n",
      "photo          : 0.9701\n",
      "art_painting   : 0.7902\n",
      "cartoon        : 0.7783\n",
      "Average LODO   : 0.7765\n",
      "All-domains baseline: 0.9195\n",
      "  sketch         : 0.8995\n",
      "  photo          : 0.9611\n",
      "  art_painting   : 0.9024\n",
      "  cartoon        : 0.9382\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Baseline: Train & Validate on All Domains ===\")\n",
    "\n",
    "# Train loader with all domains\n",
    "all_train_datasets = [pacs_data.get_dataloader(domain=d, train=True) for d in ALL_DOMAINS]\n",
    "train_loader_all = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.ConcatDataset([d.dataset for d in all_train_datasets]),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation loaders (per domain and all combined)\n",
    "val_loaders_per_domain = {d: pacs_data.get_dataloader(domain=d, train=False) for d in ALL_DOMAINS}\n",
    "all_val_datasets = [pacs_data.get_dataloader(domain=d, train=False) for d in ALL_DOMAINS]\n",
    "val_loader_all = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.ConcatDataset([d.dataset for d in all_val_datasets]),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Model (no freezing)\n",
    "baseline_model = ViTForImageClassification.from_pretrained(\n",
    "    \"WinKawaks/vit-tiny-patch16-224\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(baseline_model.parameters(), lr=LR, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_val_per_domain = {}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    # ---- Train ----\n",
    "    baseline_model.train()\n",
    "    running_loss, running_corrects, running_samples = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in train_loader_all:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = baseline_model(images)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += (preds == labels).sum().item()\n",
    "        running_samples += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / running_samples\n",
    "    train_acc = running_corrects / running_samples\n",
    "\n",
    "    # ---- Validation (all domains combined) ----\n",
    "    baseline_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_all:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = baseline_model(images)\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    # ---- Validation (per domain) ----\n",
    "    per_domain_accs = {}\n",
    "    baseline_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for domain, loader in val_loaders_per_domain.items():\n",
    "            correct, total = 0, 0\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = baseline_model(images)\n",
    "                preds = outputs.logits.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            per_domain_accs[domain] = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Acc (All Domains Combined): {val_acc:.4f}\")\n",
    "    for domain, acc in per_domain_accs.items():\n",
    "        print(f\"  {domain:15}: {acc:.4f}\")\n",
    "\n",
    "    # Track best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_val_per_domain = per_domain_accs.copy()\n",
    "\n",
    "# ---------------- Results Summary ----------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LODO RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for domain, acc in lodo_results.items():\n",
    "    print(f\"{domain:15}: {acc:.4f}\")\n",
    "\n",
    "avg_lodo = sum(lodo_results.values()) / len(lodo_results)\n",
    "print(f\"{'Average LODO':15}: {avg_lodo:.4f}\")\n",
    "print(f\"{'All-domains baseline':15}: {best_val_acc:.4f}\")\n",
    "for domain, acc in best_val_per_domain.items():\n",
    "    print(f\"  {domain:15}: {acc:.4f}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a55da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "results = {\n",
    "    \"photo\": 0.9581,\n",
    "    \"art_painting\": 0.7293,\n",
    "    \"cartoon\": 0.7186,\n",
    "    \"sketch\": 0.3461,\n",
    "    \"All-domains baseline\": 0.9010\n",
    "}\n",
    "\n",
    "# Prepare labels & values; wrap long label onto two lines for neatness\n",
    "labels = []\n",
    "values = []\n",
    "for k, v in results.items():\n",
    "    if \"All-domains\" in k:\n",
    "        labels.append(\"All-domains\\nbaseline\")   # wrap long label\n",
    "    else:\n",
    "        labels.append(k.replace(\"_\", \" \"))       # nicer display for underscores\n",
    "    values.append(v)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "x = range(len(labels))\n",
    "\n",
    "# Draw bars; make baseline visually distinct\n",
    "colors = [\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B2\", \"#888888\"]\n",
    "bars = ax.bar(x, values, color=colors, edgecolor=\"black\", linewidth=0.7)\n",
    "\n",
    "# Axis and ticks\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=11)\n",
    "ax.set_title(\"LODO Results by Domain\", fontsize=13, weight=\"bold\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, fontsize=10, rotation=0, ha=\"center\")\n",
    "\n",
    "# Add horizontal grid lines for readability (below bars)\n",
    "ax.yaxis.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Annotate values above bars with consistent alignment\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height + 0.02,\n",
    "        f\"{val:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "        fontweight=\"semibold\"\n",
    "    )\n",
    "\n",
    "# Tidy up spines\n",
    "for spine in (\"top\", \"right\"):\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save optionally:\n",
    "# fig.savefig(\"lodo_results_bar.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
