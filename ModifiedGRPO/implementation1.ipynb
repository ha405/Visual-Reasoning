{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d3527f",
   "metadata": {},
   "source": [
    "# PACS Domain Generalization with Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0ecd8",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a796be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Corrected import: Use an alias for the imported ViTModel to avoid name collision\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, ViTModel as ViTModelBase\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639f1eb",
   "metadata": {},
   "source": [
    "## Configuration and Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5145ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "SEED = 42\n",
    "BATCH_SIZE = 24\n",
    "NUM_EPOCHS = 5\n",
    "NUM_CLASSES = 7\n",
    "DATA_ROOT = \"../../../pacs_data/pacs_data\"\n",
    "DOMAINS = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n",
    "MODELS = {\n",
    "    \"base\": \"google/vit-base-patch16-224-in21k\",\n",
    "    \"small\": \"WinKawaks/vit-small-patch16-224\",\n",
    "    \"tiny\": \"WinKawaks/vit-tiny-patch16-224\"\n",
    "    }\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b48ac5",
   "metadata": {},
   "source": [
    "## Dataset Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9409c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PACSDataset:\n",
    "    def __init__(self, data_root, domains, transform):\n",
    "        self.data_root = data_root\n",
    "        self.domains = domains\n",
    "        self.transform = transform\n",
    "\n",
    "    def get_dataloader(self, domain, train=True):\n",
    "        dataset = datasets.ImageFolder(os.path.join(self.data_root, domain), transform=self.transform)\n",
    "        \n",
    "        indices = list(range(len(dataset)))\n",
    "        train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=[dataset.targets[i] for i in indices], random_state=SEED)\n",
    "        selected_idx = train_idx if train else val_idx\n",
    "        \n",
    "        subset = Subset(dataset, selected_idx)\n",
    "        loader = DataLoader(subset, batch_size=BATCH_SIZE, shuffle=train)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9cc55",
   "metadata": {},
   "source": [
    "## Vision Transformer Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23595373",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTModel(nn.Module):\n",
    "    def __init__(self, num_classes, model_size=\"base\"):\n",
    "        super(ViTModel, self).__init__()\n",
    "        # Change 1: Load the base ViTModel using the alias 'ViTModelBase'\n",
    "        self.model = ViTModelBase.from_pretrained(\n",
    "            MODELS[model_size]\n",
    "        )\n",
    "        \n",
    "        # Change 2: Define our custom \"thinking\" head\n",
    "        hidden_size = self.model.config.hidden_size # This is 768 for the base model\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Change 3: Update the forward pass logic\n",
    "        # Pass input through the base model\n",
    "        outputs = self.model(x)\n",
    "        # Get the feature vector for the [CLS] token\n",
    "        cls_token_features = outputs.last_hidden_state[:, 0, :]\n",
    "        # Pass the features through our custom head\n",
    "        logits = self.classifier_head(cls_token_features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d86814",
   "metadata": {},
   "source": [
    "## Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c203fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion):\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Change 1: Manually set the dropout layers in our custom head to train mode\n",
    "        # This keeps them active during evaluation for Monte Carlo Dropout\n",
    "        for module in self.model.classifier_head.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "\n",
    "        total_correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "                # Change 2: Perform N=20 forward passes to get an ensemble of predictions\n",
    "                ensemble_preds = []\n",
    "                for _ in range(20): # N=20 passes\n",
    "                    outputs = self.model(inputs)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    ensemble_preds.append(preds.unsqueeze(0))\n",
    "                \n",
    "                # Change 3: Calculate the majority vote for the final prediction\n",
    "                stacked_preds = torch.cat(ensemble_preds, dim=0)\n",
    "                final_preds, _ = torch.mode(stacked_preds, dim=0)\n",
    "\n",
    "                total_correct += (final_preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return total_correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348587af",
   "metadata": {},
   "source": [
    "# Stochastic Ensemble ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c03ea",
   "metadata": {},
   "source": [
    "## Leave-One-Domain-Out (LODO) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0eba20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing on domain: art_painting\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [01:21<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7092 | Val Acc: 0.9339\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [00:32<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1283 | Val Acc: 0.9207\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [00:32<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0616 | Val Acc: 0.9547\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [00:32<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0558 | Val Acc: 0.9559\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [00:32<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0387 | Val Acc: 0.9629\n",
      "Test Accuracy on art_painting: 0.8927\n",
      "\n",
      "Testing on domain: cartoon\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 255/255 [00:38<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7122 | Val Acc: 0.9471\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 255/255 [00:30<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1433 | Val Acc: 0.9542\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 255/255 [00:30<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0777 | Val Acc: 0.9471\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 255/255 [00:30<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0694 | Val Acc: 0.9399\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 255/255 [00:30<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0399 | Val Acc: 0.9542\n",
      "Test Accuracy on cartoon: 0.8060\n",
      "\n",
      "Testing on domain: photo\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "results_base = {}\n",
    "\n",
    "for test_domain in DOMAINS:\n",
    "    print(f\"\\nTesting on domain: {test_domain}\")\n",
    "    train_domains = [d for d in DOMAINS if d != test_domain]\n",
    "\n",
    "    # Load datasets\n",
    "    dataset = PACSDataset(DATA_ROOT, DOMAINS, transform)\n",
    "    train_loaders = [dataset.get_dataloader(d, train=True) for d in train_domains]\n",
    "    val_loaders = [dataset.get_dataloader(d, train=False) for d in train_domains]\n",
    "    test_loader = dataset.get_dataloader(test_domain, train=False)\n",
    "\n",
    "    # Concatenate datasets\n",
    "    train_ds = ConcatDataset([dl.dataset for dl in train_loaders])\n",
    "    val_ds = ConcatDataset([dl.dataset for dl in val_loaders])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Initialize model, optimizer, and criterion\n",
    "    model_base = ViTModel(NUM_CLASSES, model_size=\"base\")\n",
    "    optimizer = optim.Adam(model_base.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model_base, optimizer, criterion)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        train_loss_base = trainer.train(train_loader)\n",
    "        val_acc_base = trainer.evaluate(val_loader)\n",
    "        print(f\"Train Loss: {train_loss_base:.4f} | Val Acc: {val_acc_base:.4f}\")\n",
    "\n",
    "    # Test\n",
    "    test_acc_base = trainer.evaluate(test_loader)\n",
    "    results_base[test_domain] = test_acc_base\n",
    "    print(f\"Test Accuracy on {test_domain}: {test_acc_base:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73caeca0",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\Baseline: training on all domains and testing on mixed domains\")\n",
    "# Load full train and test sets via leave-all-in loaders\n",
    "dataset_all = PACSDataset(DATA_ROOT, DOMAINS, transform)\n",
    "all_train_loaders = [dataset_all.get_dataloader(d, train=True) for d in DOMAINS]\n",
    "all_test_loaders = [dataset_all.get_dataloader(d, train=False) for d in DOMAINS]\n",
    "\n",
    "# Concatenate\n",
    "full_train_ds = ConcatDataset([dl.dataset for dl in all_train_loaders])\n",
    "full_test_ds = ConcatDataset([dl.dataset for dl in all_test_loaders])\n",
    "full_train_loader = DataLoader(full_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "full_test_loader = DataLoader(full_test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize baseline model\n",
    "baseline_model_base = ViTModel(NUM_CLASSES, model_size=\"base\")\n",
    "baseline_optimizer = optim.Adam(baseline_model_base.parameters(), lr=1e-4)\n",
    "baseline_criterion = nn.CrossEntropyLoss()\n",
    "baseline_trainer = Trainer(baseline_model_base, baseline_optimizer, baseline_criterion)\n",
    "\n",
    "# Train baseline\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Baseline Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    baseline_loss_base = baseline_trainer.train(full_train_loader)\n",
    "    baseline_val_acc_base = baseline_trainer.evaluate(full_test_loader)\n",
    "    print(f\"Baseline Loss: {baseline_loss_base:.4f} | Baseline Acc: {baseline_val_acc_base:.4f}\")\n",
    "\n",
    "# Test baseline\n",
    "baseline_test_acc_base = baseline_trainer.evaluate(full_test_loader)\n",
    "results_base['baseline_all_domains'] = baseline_test_acc_base\n",
    "print(f\"Baseline Test Accuracy: {baseline_test_acc_base:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765a7ba",
   "metadata": {},
   "source": [
    "## Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = list(results_base.keys())\n",
    "accuracies = [results_base[d] for d in domains]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(domains, accuracies)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Leave-One-Domain-Out vs. Baseline Accuracy (ViT Base)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05d964",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\Final Results (LODO Accuracy):\")\n",
    "for domain, acc in results_base.items():\n",
    "    print(f\"{domain}: {acc:.4f}\")\n",
    "\n",
    "avg_acc = sum(results_base.values()) / len(results_base)\n",
    "print(f\"\\nAverage Accuracy: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a05310",
   "metadata": {},
   "source": [
    "# WinKawaks/ViT Small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0505740",
   "metadata": {},
   "source": [
    "Leave-One-Domain-Out (LODO) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f491e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "results_small = {}\n",
    "\n",
    "for test_domain in DOMAINS:\n",
    "    print(f\"\\Testing on domain: {test_domain}\")\n",
    "    train_domains = [d for d in DOMAINS if d != test_domain]\n",
    "\n",
    "    # Load datasets\n",
    "    dataset = PACSDataset(DATA_ROOT, DOMAINS, transform)\n",
    "    train_loaders = [dataset.get_dataloader(d, train=True) for d in train_domains]\n",
    "    val_loaders = [dataset.get_dataloader(d, train=False) for d in train_domains]\n",
    "    test_loader = dataset.get_dataloader(test_domain, train=False)\n",
    "\n",
    "    # Concatenate datasets\n",
    "    train_ds = ConcatDataset([dl.dataset for dl in train_loaders])\n",
    "    val_ds = ConcatDataset([dl.dataset for dl in val_loaders])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Initialize model, optimizer, and criterion\n",
    "    model_small = ViTModel(NUM_CLASSES, model_size=\"small\")\n",
    "    optimizer = optim.Adam(model_small.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model_small, optimizer, criterion)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        train_loss_small = trainer.train(train_loader)\n",
    "        val_acc_small = trainer.evaluate(val_loader)\n",
    "        print(f\"Train Loss: {train_loss_small:.4f} | Val Acc: {val_acc_small:.4f}\")\n",
    "\n",
    "    # Test\n",
    "    test_acc_small = trainer.evaluate(test_loader)\n",
    "    results_small[test_domain] = test_acc_small\n",
    "    print(f\"Test Accuracy on {test_domain}: {test_acc_small:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace326a2",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBaseline: training on all domains and testing on mixed domains\")\n",
    "# Load full train and test sets via leave-all-in loaders\n",
    "dataset_all = PACSDataset(DATA_ROOT, DOMAINS, transform)\n",
    "all_train_loaders = [dataset_all.get_dataloader(d, train=True) for d in DOMAINS]\n",
    "all_test_loaders = [dataset_all.get_dataloader(d, train=False) for d in DOMAINS]\n",
    "\n",
    "# Concatenate\n",
    "full_train_ds = ConcatDataset([dl.dataset for dl in all_train_loaders])\n",
    "full_test_ds = ConcatDataset([dl.dataset for dl in all_test_loaders])\n",
    "full_train_loader = DataLoader(full_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "full_test_loader = DataLoader(full_test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize baseline model\n",
    "# CORRECTED: Added model_size=\"small\" to ensure the correct model is loaded.\n",
    "baseline_model_small = ViTModel(NUM_CLASSES, model_size=\"small\")\n",
    "baseline_optimizer = optim.Adam(baseline_model_small.parameters(), lr=1e-4)\n",
    "baseline_criterion = nn.CrossEntropyLoss()\n",
    "baseline_trainer = Trainer(baseline_model_small, baseline_optimizer, baseline_criterion)\n",
    "\n",
    "# Train baseline\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Baseline Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    baseline_loss_small = baseline_trainer.train(full_train_loader)\n",
    "    baseline_val_acc_small = baseline_trainer.evaluate(full_test_loader)\n",
    "    print(f\"Baseline Loss: {baseline_loss_small:.4f} | Baseline Acc: {baseline_val_acc_small:.4f}\")\n",
    "\n",
    "# Test baseline\n",
    "baseline_test_acc_small = baseline_trainer.evaluate(full_test_loader)\n",
    "results_small['baseline_all_domains'] = baseline_test_acc_small\n",
    "print(f\"Baseline Test Accuracy: {baseline_test_acc_small:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba7758",
   "metadata": {},
   "source": [
    "## Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4dd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = list(results_small.keys())\n",
    "accuracies = [results_small[d] for d in domains]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(domains, accuracies)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Leave-One-Domain-Out vs. Baseline Accuracy (ViT Small)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e5e15",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\Final Results (LODO Accuracy):\")\n",
    "for domain, acc in results_small.items():\n",
    "    print(f\"{domain}: {acc:.4f}\")\n",
    "\n",
    "avg_acc = sum(results_small.values()) / len(results_small)\n",
    "print(f\"\\nAverage Accuracy: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b32b45",
   "metadata": {},
   "source": [
    "# WinKawaks/ViT Tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ced9ba",
   "metadata": {},
   "source": [
    "## Leave-One-Domain-Out (LODO) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WinKawaks/ViT Tiny\n",
    "# Leave-One-Domain-Out (LODO) Training\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "results_tiny = {}\n",
    "\n",
    "for test_domain in DOMAINS:\n",
    "    print(f\"\\nTesting on domain: {test_domain}\")\n",
    "    train_domains = [d for d in DOMAINS if d != test_domain]\n",
    "\n",
    "    # Load datasets\n",
    "    dataset = PACSDataset(DATA_ROOT, DOMAINS, transform)\n",
    "    train_loaders = [dataset.get_dataloader(d, train=True) for d in train_domains]\n",
    "    val_loaders = [dataset.get_dataloader(d, train=False) for d in train_domains]\n",
    "    test_loader = dataset.get_dataloader(test_domain, train=False)\n",
    "\n",
    "    # Concatenate datasets\n",
    "    train_ds = ConcatDataset([dl.dataset for dl in train_loaders])\n",
    "    val_ds = ConcatDataset([dl.dataset for dl in val_loaders])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Initialize model, optimizer, and criterion\n",
    "    model_tiny = ViTModel(NUM_CLASSES, model_size=\"tiny\")\n",
    "    # CORRECTED: Optimizer now uses parameters from model_tiny, not model_base.\n",
    "    optimizer = optim.Adam(model_tiny.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model_tiny, optimizer, criterion)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        train_loss_tiny = trainer.train(train_loader)\n",
    "        val_acc_tiny = trainer.evaluate(val_loader)\n",
    "        print(f\"Train Loss: {train_loss_tiny:.4f} | Val Acc: {val_acc_tiny:.4f}\")\n",
    "\n",
    "    # Test\n",
    "    test_acc_tiny = trainer.evaluate(test_loader)\n",
    "    results_tiny[test_domain] = test_acc_tiny\n",
    "    print(f\"Test Accuracy on {test_domain}: {test_acc_tiny:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8805d",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f77dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBaseline: training on all domains and testing on mixed domains\")\n",
    "# Load full train and test sets via leave-all-in loaders\n",
    "dataset_all = PACSDataset(DATA_ROOT, DOMAINS, transform)\n",
    "all_train_loaders = [dataset_all.get_dataloader(d, train=True) for d in DOMAINS]\n",
    "all_test_loaders = [dataset_all.get_dataloader(d, train=False) for d in DOMAINS]\n",
    "\n",
    "# Concatenate\n",
    "full_train_ds = ConcatDataset([dl.dataset for dl in all_train_loaders])\n",
    "full_test_ds = ConcatDataset([dl.dataset for dl in all_test_loaders])\n",
    "full_train_loader = DataLoader(full_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "full_test_loader = DataLoader(full_test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize baseline model\n",
    "# CORRECTED: Added model_size=\"tiny\" to ensure the correct model is loaded.\n",
    "baseline_model_tiny = ViTModel(NUM_CLASSES, model_size=\"tiny\")\n",
    "baseline_optimizer = optim.Adam(baseline_model_tiny.parameters(), lr=1e-4)\n",
    "baseline_criterion = nn.CrossEntropyLoss()\n",
    "baseline_trainer = Trainer(baseline_model_tiny, baseline_optimizer, baseline_criterion)\n",
    "\n",
    "# Train baseline\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Baseline Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    baseline_loss_tiny = baseline_trainer.train(full_train_loader)\n",
    "    baseline_val_acc_tiny = baseline_trainer.evaluate(full_test_loader)\n",
    "    print(f\"Baseline Loss: {baseline_loss_tiny:.4f} | Baseline Acc: {baseline_val_acc_tiny:.4f}\")\n",
    "\n",
    "# Test baseline\n",
    "baseline_test_acc_tiny = baseline_trainer.evaluate(full_test_loader)\n",
    "results_tiny['baseline_all_domains'] = baseline_test_acc_tiny\n",
    "print(f\"Baseline Test Accuracy: {baseline_test_acc_tiny:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac02d52",
   "metadata": {},
   "source": [
    "## Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = list(results_tiny.keys())\n",
    "accuracies = [results_tiny[d] for d in domains]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(domains, accuracies)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Leave-One-Domain-Out vs. Baseline Accuracy (ViT Tiny)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a667907",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\Final Results (LODO Accuracy):\")\n",
    "for domain, acc in results_tiny.items():\n",
    "    print(f\"{domain}: {acc:.4f}\")\n",
    "\n",
    "avg_acc = sum(results_tiny.values()) / len(results_tiny)\n",
    "print(f\"\\nAverage Accuracy: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee3a188",
   "metadata": {},
   "source": [
    "# Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for idx, domain in enumerate(DOMAINS):\n",
    "    plt.subplot(2, 3, idx + 1)\n",
    "\n",
    "    domain_accuracies = [\n",
    "        results_base[domain],\n",
    "        results_small[domain],\n",
    "        results_tiny[domain]\n",
    "    ]\n",
    "\n",
    "    bars = plt.bar(MODELS.keys(), domain_accuracies, color=[\"skyblue\", \"orange\", \"green\"])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.title(f\"Model Comparison - {domain} Domain\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, acc in enumerate(domain_accuracies):\n",
    "        plt.text(i, acc + 0.01, f\"{acc:.2%}\", ha=\"center\")\n",
    "\n",
    "# Create the baseline comparison subplot\n",
    "plt.subplot(2, 3, 5) \n",
    "baseline_accuracies = [\n",
    "    results_base[\"baseline_all_domains\"],\n",
    "    results_small[\"baseline_all_domains\"],\n",
    "    results_tiny[\"baseline_all_domains\"]\n",
    "]\n",
    "plt.bar(MODELS.keys(), baseline_accuracies, color=[\"skyblue\", \"orange\", \"green\"])\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Model Comparison - Baseline (All Domains)\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for i, acc in enumerate(baseline_accuracies):\n",
    "    plt.text(i, acc + 0.01, f\"{acc:.2%}\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDetailed Performance Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Domain':<15} {'Base':>10} {'Small':>10} {'Tiny':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    print(f\"{domain:<15} {results_base[domain]:>10.2%} {results_small[domain]:>10.2%} {results_tiny[domain]:>10.2%}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Baseline':<15} {results_base['baseline_all_domains']:>10.2%} {results_small['baseline_all_domains']:>10.2%} {results_tiny['baseline_all_domains']:>10.2%}\")\n",
    "print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
