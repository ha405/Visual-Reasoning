{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 1: PROJECT SCAFFOLDING & CONFIGURATION (BASELINE)\n",
    "# =================================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1.1: IMPORTS\n",
    "# ---------------------------------------------------------------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1.2: CONFIGURATION CLASS\n",
    "# ---------------------------------------------------------------------------------\n",
    "class Config:\n",
    "    # --- Data Paths and Domains ---\n",
    "    DATA_DIR = r\"D:\\Haseeb\\Datasets\\VLCS\"\n",
    "    DOMAINS = [\"Caltech101\", \"LabelMe\", \"SUN09\", \"VOC2007\"]\n",
    "    \n",
    "    # --- Model & Architecture ---\n",
    "    MODEL_NAME = \"WinKawaks/vit-tiny-patch16-224\"\n",
    "    NUM_CLASSES = 5\n",
    "    ### CHANGE ###\n",
    "    NUM_HEADS = 1 # A baseline model has only one classification head.\n",
    "    DROPOUT_RATE = 0.5\n",
    "    \n",
    "    # --- Training Hyperparameters ---\n",
    "    BATCH_SIZE = 128\n",
    "    NUM_EPOCHS = 5\n",
    "    LEARNING_RATE = 1e-4\n",
    "    OPTIMIZER = \"AdamW\"\n",
    "    \n",
    "    # --- Hardware & Reproducibility ---\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 42\n",
    "\n",
    "# Instantiate the config\n",
    "config = Config()\n",
    "\n",
    "# Print out the configuration to verify\n",
    "print(\"--- Project Configuration (BASELINE) ---\")\n",
    "for key, value in config.__class__.__dict__.items():\n",
    "    if not key.startswith('__'):\n",
    "        print(f\"{key}: {value}\")\n",
    "print(\"--------------------------------------\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1.3: RESULTS TRACKER\n",
    "# ---------------------------------------------------------------------------------\n",
    "experiment_results = []\n",
    "\n",
    "print(\"\\nProject scaffolding is complete. Ready for Section 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 2: DATA LOADING & PREPROCESSING (BASELINE)\n",
    "# =================================================================================\n",
    "# (This section is identical to the previous notebook)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 2.1: IMAGE TRANSFORMATIONS\n",
    "# ---------------------------------------------------------------------------------\n",
    "IMG_SIZE = 224\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# %%\n",
    "# =================================================================================\n",
    "# SECTION 2.2: CUSTOM DOMAIN GENERALIZATION DATASET CLASS\n",
    "# =================================================================================\n",
    "### CHANGE: Renamed class from PACSDataset to DomainGeneralizationDataset for clarity ###\n",
    "class DomainGeneralizationDataset(Dataset):\n",
    "    def __init__(self, root_dir, domains, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the domain folders.\n",
    "            domains (list of string): List of domains to include in this dataset.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.domains = domains\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Discover all classes from the first domain folder\n",
    "        # NOTE: Make sure the class folder names are consistent across all domain folders.\n",
    "        self.classes = sorted(os.listdir(os.path.join(root_dir, domains[0])))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Load image paths and labels from the specified domains\n",
    "        for domain in self.domains:\n",
    "            domain_path = os.path.join(self.root_dir, domain)\n",
    "            for class_name in self.classes:\n",
    "                class_path = os.path.join(domain_path, class_name)\n",
    "                # Check if the class path exists before trying to list its directory\n",
    "                if os.path.isdir(class_path):\n",
    "                    for img_name in os.listdir(class_path):\n",
    "                        self.image_paths.append(os.path.join(class_path, img_name))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            # Add a try-except block to handle potentially corrupt images\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except (IOError, OSError) as e:\n",
    "            print(f\"Warning: Skipping corrupted image: {img_path}\")\n",
    "            # Return the next valid item\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# =================================================================================\n",
    "# SECTION 2.3: DATALOADER HELPER FUNCTION (80/20 SPLIT)\n",
    "# =================================================================================\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataloaders(root_dir, target_domain, all_domains, batch_size, seed):\n",
    "    \"\"\"\n",
    "    Creates dataloaders for a LODO split using an 80/20 split on the source domains.\n",
    "    \"\"\"\n",
    "    source_domains = [d for d in all_domains if d != target_domain]\n",
    "    \n",
    "    print(f\"--- Creating DataLoaders (80/20 Split Strategy) ---\")\n",
    "    print(f\"Target (Test) Domain: {target_domain}\")\n",
    "    print(f\"Source Domains for Train/Val: {source_domains}\")\n",
    "    \n",
    "    # 1. Create a single, large dataset by combining all source domains\n",
    "    ### CHANGE: Use the new DomainGeneralizationDataset class ###\n",
    "    source_dataset = DomainGeneralizationDataset(\n",
    "        root_dir=root_dir, \n",
    "        domains=source_domains, \n",
    "        transform=data_transforms['train']\n",
    "    )\n",
    "    \n",
    "    indices = list(range(len(source_dataset)))\n",
    "    labels = source_dataset.labels\n",
    "    \n",
    "    train_idx, val_idx = train_test_split(\n",
    "        indices, \n",
    "        test_size=0.2, \n",
    "        stratify=labels, \n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # 2. Create subsets\n",
    "    train_subset = Subset(source_dataset, train_idx)\n",
    "    \n",
    "    ### CHANGE: Use the new DomainGeneralizationDataset class ###\n",
    "    val_dataset_clean = DomainGeneralizationDataset(root_dir=root_dir, domains=source_domains, transform=data_transforms['val'])\n",
    "    val_subset_final = Subset(val_dataset_clean, val_idx)\n",
    "    \n",
    "    # 3. Create the test dataset from the full target domain\n",
    "    ### CHANGE: Use the new DomainGeneralizationDataset class ###\n",
    "    test_dataset = DomainGeneralizationDataset(\n",
    "        root_dir=root_dir, \n",
    "        domains=[target_domain], \n",
    "        transform=data_transforms['val']\n",
    "    )\n",
    "\n",
    "    # 4. Create the DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_subset_final, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Source data size: {len(source_dataset)}\")\n",
    "    print(f\"  -> Training on: {len(train_subset)} images (80%)\")\n",
    "    print(f\"  -> Validating on: {len(val_subset_final)} images (20%)\")\n",
    "    print(f\"Testing on full '{target_domain}' domain: {len(test_dataset)} images\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 3: THE MODEL ARCHITECTURE (BASELINE)\n",
    "# =================================================================================\n",
    "\n",
    "### CHANGE ###\n",
    "# Renamed to BaselineViT and simplified for a single head.\n",
    "\n",
    "class BaselineViT(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate):\n",
    "        super(BaselineViT, self).__init__()\n",
    "        \n",
    "        self.vit_backbone = ViTModel.from_pretrained(model_name)\n",
    "        hidden_dim = self.vit_backbone.config.hidden_size\n",
    "        \n",
    "        ### CHANGE ###\n",
    "        # We only create a single head now, not a ModuleList.\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, images):\n",
    "        outputs = self.vit_backbone(pixel_values=images)\n",
    "        feature_vector_z = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        ### CHANGE ###\n",
    "        # The forward pass is simpler: just pass through the single head\n",
    "        # and return the final logits tensor directly.\n",
    "        logits = self.head(feature_vector_z)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 4: TRAINING & EVALUATION LOGIC (BASELINE)\n",
    "# =================================================================================\n",
    "\n",
    "### CHANGE ###\n",
    "# This is now a standard, simple training loop. No competition, no winner selection.\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training Epoch\", leave=False)\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return {\"avg_train_loss\": avg_loss}\n",
    "\n",
    "\n",
    "### CHANGE ###\n",
    "# The evaluation function is simplified. No ensembling is needed.\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            correct_preds += torch.sum(preds == labels).item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_preds / total_samples\n",
    "    \n",
    "    return {\"avg_loss\": avg_loss, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 5: THE MAIN EXPERIMENT LOOP (BASELINE)\n",
    "# =================================================================================\n",
    "\n",
    "config = Config()\n",
    "lodo_histories = {}\n",
    "\n",
    "for target_domain in config.DOMAINS:\n",
    "    print(f\"==============================================================\")\n",
    "    print(f\"  STARTING LODO EXPERIMENT: Target Domain = {target_domain.upper()}\")\n",
    "    print(f\"==============================================================\")\n",
    "    \n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        root_dir=config.DATA_DIR,\n",
    "        target_domain=target_domain,\n",
    "        all_domains=config.DOMAINS,\n",
    "        batch_size=config.BATCH_SIZE, seed=config.SEED\n",
    "    )\n",
    "    \n",
    "    ### CHANGE ###\n",
    "    # Instantiate the BaselineViT model.\n",
    "    model = BaselineViT(\n",
    "        model_name=config.MODEL_NAME,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        dropout_rate=config.DROPOUT_RATE\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.NUM_EPOCHS} ---\")\n",
    "        \n",
    "        train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, config.DEVICE)\n",
    "        val_metrics = evaluate(model, val_loader, criterion, config.DEVICE)\n",
    "        \n",
    "        ### CHANGE ###\n",
    "        # Update the logging to match the new return values from train_one_epoch.\n",
    "        print(f\"Epoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {train_metrics['avg_train_loss']:.4f}\")\n",
    "        print(f\"  Validation Loss: {val_metrics['avg_loss']:.4f}\")\n",
    "        print(f\"  Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "\n",
    "        history[\"train_loss\"].append(train_metrics['avg_train_loss'])\n",
    "        history[\"val_loss\"].append(val_metrics['avg_loss'])\n",
    "        history[\"val_accuracy\"].append(val_metrics['accuracy'])\n",
    "        \n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            print(f\"  New best validation accuracy! Saving model state.\")\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "    print(\"\\nTraining complete for this LODO split.\")\n",
    "    print(\"Loading best model state and evaluating on the TEST set...\")\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    test_metrics = evaluate(model, test_loader, criterion, config.DEVICE)\n",
    "    \n",
    "    print(f\"\\n--- RESULTS FOR TARGET DOMAIN: {target_domain.upper()} ---\")\n",
    "    print(f\"  Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    \n",
    "    lodo_histories[target_domain] = history\n",
    "    \n",
    "    experiment_results.append({\n",
    "        \"target_domain\": target_domain,\n",
    "        \"source_domains\": [d for d in config.DOMAINS if d != target_domain],\n",
    "        \"test_accuracy\": test_metrics['accuracy'],\n",
    "        \"best_val_accuracy\": best_val_accuracy,\n",
    "        \"model_name\": config.MODEL_NAME,\n",
    "        \"num_epochs\": config.NUM_EPOCHS,\n",
    "        \"batch_size\": config.BATCH_SIZE,\n",
    "        \"learning_rate\": config.LEARNING_RATE\n",
    "    })\n",
    "\n",
    "print(\"\\n\\n==============================================================\")\n",
    "print(\"          ALL BASELINE LODO EXPERIMENTS COMPLETE\")\n",
    "print(\"==============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 7: VISUALIZE LEARNING CURVES\n",
    "# =================================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Visualizing Learning Curves for Each LODO Experiment ---\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for domain, history in lodo_histories.items():\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Plotting losses on the primary y-axis\n",
    "    ax1.set_xlabel('Epochs', fontsize=14)\n",
    "    ax1.set_ylabel('Loss', fontsize=14, color='tab:blue')\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    ax1.plot(epochs, history['val_loss'], 'b--', label='Validation Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    \n",
    "    # Creating a secondary y-axis for accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Accuracy', fontsize=14, color='tab:green')\n",
    "    ax2.plot(epochs, history['val_accuracy'], 'g-s', label='Validation Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "    \n",
    "    # Adding title and legends\n",
    "    plt.title(f'Learning Curves (Target Domain: {domain.upper()})', fontsize=16, fontweight='bold')\n",
    "    fig.legend(loc=\"upper right\", bbox_to_anchor=(0.9,0.9))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 6: ANALYSIS & VISUALIZATION (with Dictionary Output)\n",
    "# =================================================================================\n",
    "# Now that all experiments are complete, we'll process the results\n",
    "# and create visualizations to understand the performance of our method.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# Add this magic command to ensure plots are displayed in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 6.1: DISPLAY RESULTS IN A TABLE\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"--- Final Experiment Results ---\")\n",
    "\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "column_order = [\n",
    "    \"target_domain\", \"test_accuracy\", \"best_val_accuracy\", \"num_epochs\",\n",
    "    \"batch_size\", \"learning_rate\", \"model_name\"\n",
    "]\n",
    "existing_columns = [col for col in column_order if col in results_df.columns]\n",
    "results_df = results_df[existing_columns]\n",
    "average_accuracy = results_df['test_accuracy'].mean()\n",
    "\n",
    "print(results_df.to_string())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Average Test Accuracy Across All Domains: {average_accuracy:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 6.2: VISUALIZE THE RESULTS\n",
    "# ---------------------------------------------------------------------------------\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    data=results_df, x='target_domain', y='test_accuracy', ax=ax, palette='viridis'\n",
    ")\n",
    "\n",
    "for index, row in results_df.iterrows():\n",
    "    ax.text(index, row['test_accuracy'] + 0.01, f\"{row['test_accuracy']:.2%}\",\n",
    "            color='black', ha=\"center\", fontsize=12)\n",
    "    \n",
    "ax.axhline(average_accuracy, ls='--', color='red', label=f'Average Accuracy ({average_accuracy:.2%})')\n",
    "\n",
    "ax.set_title('Model Performance on Unseen Target Domains (LODO)', fontsize=16, pad=20)\n",
    "ax.set_xlabel('Target (Unseen) Domain', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "### NEW SECTION ###\n",
    "# 6.3: GENERATE COPY-PASTE DICTIONARY FOR FINAL PLOTTING\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Dictionary for Final Plotting ---\")\n",
    "print(\"# Copy the dictionary below and paste it into your final analysis notebook.\")\n",
    "\n",
    "# Determine the variable name based on the notebook (you can adjust this)\n",
    "# For the baseline notebook, you'd want 'baseline_results'.\n",
    "# For the evolutionary notebook, you'd want 'evolutionary_results'.\n",
    "method_name = \"my_method_results\" # Generic name\n",
    "if \"baseline\" in os.getcwd(): # Simple check if 'baseline' is in the notebook path\n",
    "    method_name = \"baseline_results\"\n",
    "elif \"drop-out\" in os.getcwd():\n",
    "    method_name = \"evolutionary_results\"\n",
    "    \n",
    "# Extract the lists from the DataFrame\n",
    "domain_list = results_df['target_domain'].tolist()\n",
    "accuracy_list = [round(acc, 4) for acc in results_df['test_accuracy'].tolist()]\n",
    "\n",
    "# Print in the desired format\n",
    "print(f\"{method_name} = {{\")\n",
    "print(f\"    'target_domain': {domain_list},\")\n",
    "print(f\"    'test_accuracy': {accuracy_list}\")\n",
    "print(f\"}}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "print(\"\\n--- Experiment Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8446344,
     "sourceId": 13323122,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
