{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 1: PROJECT SCAFFOLDING & CONFIGURATION\n",
    "# =================================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1.1: IMPORTS\n",
    "# ---------------------------------------------------------------------------------\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =================================================================================\n",
    "# SECTION 1.2: CONFIGURATION CLASS (FOR EMA WEIGHT TRANSFER ON VLCS)\n",
    "# =================================================================================\n",
    "class Config:\n",
    "    # --- Data Paths and Domains ---\n",
    "    ### CHANGE 1: Update the DATA_DIR to your VLCS path ###\n",
    "    DATA_DIR = r\"D:\\Haseeb\\Datasets\\VLCS\"\n",
    "    ### CHANGE 2: Update the DOMAINS list for VLCS ###\n",
    "    DOMAINS = [\"Caltech101\", \"LabelMe\", \"SUN09\", \"VOC2007\"]\n",
    "    \n",
    "    # --- Model & Architecture ---\n",
    "    MODEL_NAME = \"WinKawaks/vit-tiny-patch16-224\"\n",
    "    ### CHANGE 3: Update the NUM_CLASSES for VLCS ###\n",
    "    NUM_CLASSES = 5 \n",
    "    NUM_HEADS = 4\n",
    "    DROPOUT_OPTIONS = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    \n",
    "    # --- EMA Hyperparameter ---\n",
    "    EMA_DECAY = 0.999\n",
    "    \n",
    "    # --- Training Hyperparameters ---\n",
    "    BATCH_SIZE = 128\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 1e-4\n",
    "    OPTIMIZER = \"AdamW\"\n",
    "    \n",
    "    # --- Hardware & Reproducibility ---\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 42\n",
    "\n",
    "# Instantiate the config\n",
    "config = Config()\n",
    "\n",
    "print(\"--- Project Configuration (EMA on VLCS) ---\")\n",
    "for key, value in config.__class__.__dict__.items():\n",
    "    if not key.startswith('__'):\n",
    "        print(f\"{key}: {value}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1.3: RESULTS TRACKER\n",
    "# ---------------------------------------------------------------------------------\n",
    "experiment_results = []\n",
    "\n",
    "print(\"\\nProject scaffolding is complete. Ready for Section 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 2: DATA LOADING & PREPROCESSING\n",
    "# =================================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 2.1: IMAGE TRANSFORMATIONS\n",
    "# Define the transformations for training (with augmentation) and validation/testing.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# The ViT model was pre-trained on images of size 224x224\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# The normalization values are standard for many pre-trained models\n",
    "# but it's good practice to use the ones specified by the model's authors if available.\n",
    "# For ViT, a simple (0.5, 0.5, 0.5) normalization is common.\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(), # A simple data augmentation technique\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =================================================================================\n",
    "# SECTION 2.2: CUSTOM DOMAIN GENERALIZATION DATASET CLASS\n",
    "# =================================================================================\n",
    "### CHANGE: Renamed class from PACSDataset to DomainGeneralizationDataset for clarity ###\n",
    "class DomainGeneralizationDataset(Dataset):\n",
    "    def __init__(self, root_dir, domains, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the domain folders.\n",
    "            domains (list of string): List of domains to include in this dataset.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.domains = domains\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Discover all classes from the first domain folder\n",
    "        # NOTE: Make sure the class folder names are consistent across all domain folders.\n",
    "        self.classes = sorted(os.listdir(os.path.join(root_dir, domains[0])))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Load image paths and labels from the specified domains\n",
    "        for domain in self.domains:\n",
    "            domain_path = os.path.join(self.root_dir, domain)\n",
    "            for class_name in self.classes:\n",
    "                class_path = os.path.join(domain_path, class_name)\n",
    "                # Check if the class path exists before trying to list its directory\n",
    "                if os.path.isdir(class_path):\n",
    "                    for img_name in os.listdir(class_path):\n",
    "                        self.image_paths.append(os.path.join(class_path, img_name))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            # Add a try-except block to handle potentially corrupt images\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except (IOError, OSError) as e:\n",
    "            print(f\"Warning: Skipping corrupted image: {img_path}\")\n",
    "            # Return the next valid item\n",
    "            return self.__getitem__((idx + 1) % len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =================================================================================\n",
    "# SECTION 2.3: DATALOADER HELPER FUNCTION (80/20 SPLIT)\n",
    "# =================================================================================\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataloaders(root_dir, target_domain, all_domains, batch_size, seed):\n",
    "    \"\"\"\n",
    "    Creates dataloaders for a LODO split using an 80/20 split on the source domains.\n",
    "    \"\"\"\n",
    "    source_domains = [d for d in all_domains if d != target_domain]\n",
    "    \n",
    "    print(f\"--- Creating DataLoaders (80/20 Split Strategy) ---\")\n",
    "    print(f\"Target (Test) Domain: {target_domain}\")\n",
    "    print(f\"Source Domains for Train/Val: {source_domains}\")\n",
    "    \n",
    "    # 1. Create a single, large dataset by combining all source domains\n",
    "    ### CHANGE: Use the new DomainGeneralizationDataset class ###\n",
    "    source_dataset = DomainGeneralizationDataset(\n",
    "        root_dir=root_dir, \n",
    "        domains=source_domains, \n",
    "        transform=data_transforms['train']\n",
    "    )\n",
    "    \n",
    "    indices = list(range(len(source_dataset)))\n",
    "    labels = source_dataset.labels\n",
    "    \n",
    "    train_idx, val_idx = train_test_split(\n",
    "        indices, \n",
    "        test_size=0.2, \n",
    "        stratify=labels, \n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # 2. Create subsets\n",
    "    train_subset = Subset(source_dataset, train_idx)\n",
    "    \n",
    "    ### CHANGE: Use the new DomainGeneralizationDataset class ###\n",
    "    val_dataset_clean = DomainGeneralizationDataset(root_dir=root_dir, domains=source_domains, transform=data_transforms['val'])\n",
    "    val_subset_final = Subset(val_dataset_clean, val_idx)\n",
    "    \n",
    "    # 3. Create the test dataset from the full target domain\n",
    "    ### CHANGE: Use the new DomainGeneralizationDataset class ###\n",
    "    test_dataset = DomainGeneralizationDataset(\n",
    "        root_dir=root_dir, \n",
    "        domains=[target_domain], \n",
    "        transform=data_transforms['val']\n",
    "    )\n",
    "\n",
    "    # 4. Create the DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_subset_final, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Source data size: {len(source_dataset)}\")\n",
    "    print(f\"  -> Training on: {len(train_subset)} images (80%)\")\n",
    "    print(f\"  -> Validating on: {len(val_subset_final)} images (20%)\")\n",
    "    print(f\"Testing on full '{target_domain}' domain: {len(test_dataset)} images\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 3: THE MODEL ARCHITECTURE (EMA WEIGHT TRANSFER)\n",
    "# =================================================================================\n",
    "\n",
    "class DistillationViT(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rates: list, dummy_dropout_rate: float):\n",
    "        super(DistillationViT, self).__init__()\n",
    "        \n",
    "        self.vit_backbone = ViTModel.from_pretrained(model_name)\n",
    "        hidden_dim = self.vit_backbone.config.hidden_size\n",
    "        \n",
    "        # The 4 \"teacher\" heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Dropout(p=rate),\n",
    "                nn.Linear(hidden_dim, num_classes)\n",
    "            ) for rate in dropout_rates\n",
    "        ])\n",
    "        \n",
    "        ### CRITICAL CHANGE ###\n",
    "        # The \"student\" head MUST have the same architecture as the teachers\n",
    "        # so that their weights can be averaged.\n",
    "        self.dummy_head = nn.Sequential(\n",
    "            nn.Dropout(p=dummy_dropout_rate),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        # We initialize the dummy head with the same weights as the first teacher head\n",
    "        self.dummy_head.load_state_dict(self.heads[0].state_dict())\n",
    "\n",
    "\n",
    "    def update_dropout_rates(self, new_rates: list):\n",
    "        for i, head in enumerate(self.heads):\n",
    "            head[0].p = new_rates[i]\n",
    "            \n",
    "    def update_dummy_dropout_rate(self, new_rate: float):\n",
    "        self.dummy_head[0].p = new_rate\n",
    "        \n",
    "    def forward(self, images):\n",
    "        z = self.vit_backbone(pixel_values=images).last_hidden_state[:, 0, :]\n",
    "        return z\n",
    "\n",
    "print(\"EMA-ready DistillationViT class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 4: TRAINING & EVALUATION LOGIC (WEIGHTED EMA + FULL DIAGNOSTICS)\n",
    "# =================================================================================\n",
    "\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    # Trackers for ALL losses (used for plotting)\n",
    "    batch_backbone_losses = []\n",
    "    batch_dummy_losses = []\n",
    "    batch_head_losses = [[] for _ in range(len(model.heads))]\n",
    "    \n",
    "    head_correct_preds = defaultdict(int)\n",
    "    total_samples = 0\n",
    "\n",
    "    # Temporary holder for weighted average teacher\n",
    "    with torch.no_grad():\n",
    "        weighted_avg_teacher = copy.deepcopy(model.dummy_head)\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training Epoch\", leave=False)\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        total_samples += len(labels)\n",
    "        \n",
    "        z = model(images)\n",
    "        z_detached = z.detach()\n",
    "\n",
    "        # --- PHASE 1: DIAGNOSTICS (Calculate all losses for plotting) ---\n",
    "        with torch.no_grad():\n",
    "            # 1. Dummy Head Diagnostic Loss\n",
    "            dummy_logits = model.dummy_head(z)\n",
    "            batch_dummy_losses.append(classification_criterion(dummy_logits, labels).item())\n",
    "            \n",
    "            # 2. All Teacher Heads Diagnostic Losses (on live z for fair comparison)\n",
    "            for i, head in enumerate(model.heads):\n",
    "                head_logits_diag = head(z)\n",
    "                batch_head_losses[i].append(classification_criterion(head_logits_diag, labels).item())\n",
    "\n",
    "        # --- PHASE 2: TRAINING (The actual work) ---\n",
    "        head_logits = {f'head_{i+1}': head(z) for i, head in enumerate(model.heads)}\n",
    "\n",
    "        # Find the winner\n",
    "        batch_accuracies = {}\n",
    "        for head_name, logits in head_logits.items():\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            correct = torch.sum(preds == labels).item()\n",
    "            batch_accuracies[head_name] = correct / len(labels)\n",
    "            head_correct_preds[head_name] += correct\n",
    "        winner_head_name = max(batch_accuracies, key=batch_accuracies.get)\n",
    "\n",
    "        # Calculate Training Losses\n",
    "        winner_loss = classification_criterion(head_logits[winner_head_name], labels)\n",
    "        batch_backbone_losses.append(winner_loss.item()) # Winner drives backbone loss\n",
    "\n",
    "        losers_loss = 0\n",
    "        for i, head in enumerate(model.heads):\n",
    "            if i != (int(winner_head_name[-1])-1):\n",
    "                loser_logits = head(z_detached)\n",
    "                losers_loss += classification_criterion(loser_logits, labels)\n",
    "        \n",
    "        final_loss = winner_loss + losers_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        final_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # --- PHASE 3: EMA UPDATE ---\n",
    "        with torch.no_grad():\n",
    "            accuracies_tensor = torch.tensor(list(batch_accuracies.values()), device=device)\n",
    "            accuracy_weights = torch.nn.functional.softmax(accuracies_tensor, dim=0)\n",
    "\n",
    "            for param in weighted_avg_teacher.parameters():\n",
    "                param.data.zero_()\n",
    "            \n",
    "            for i, teacher_head in enumerate(model.heads):\n",
    "                weight = accuracy_weights[i]\n",
    "                for avg_param, teacher_param in zip(weighted_avg_teacher.parameters(), teacher_head.parameters()):\n",
    "                    avg_param.data.add_(teacher_param.data, alpha=weight)\n",
    "\n",
    "            for student_param, avg_teacher_param in zip(model.dummy_head.parameters(), weighted_avg_teacher.parameters()):\n",
    "                student_param.data.mul_(config.EMA_DECAY).add_(avg_teacher_param.data, alpha=1 - config.EMA_DECAY)\n",
    "\n",
    "    # Collate all metrics for return\n",
    "    epoch_metrics = {\n",
    "        \"avg_backbone_loss\": np.mean(batch_backbone_losses),\n",
    "        \"avg_dummy_loss\": np.mean(batch_dummy_losses), # Now correctly populated\n",
    "        \"head_accuracies\": {name: correct / total_samples for name, correct in head_correct_preds.items()}\n",
    "    }\n",
    "    for i in range(len(model.heads)):\n",
    "        epoch_metrics[f\"avg_head_{i+1}_loss\"] = np.mean(batch_head_losses[i])\n",
    "        \n",
    "    return epoch_metrics\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct_preds, total_samples = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            total_samples += len(labels)\n",
    "            z = model(images)\n",
    "            dummy_logits = model.dummy_head(z)\n",
    "            loss = classification_criterion(dummy_logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(dummy_logits, 1)\n",
    "            correct_preds += torch.sum(preds == labels).item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_preds / total_samples\n",
    "    return {\"avg_loss\": avg_loss, \"accuracy\": accuracy}\n",
    "\n",
    "print(\"Diagnostic-ready train_one_epoch and evaluate functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 5: THE MAIN EXPERIMENT LOOP (WEIGHTED EMA + DIAGNOSTICS)\n",
    "# =================================================================================\n",
    "\n",
    "config = Config()\n",
    "lodo_histories = {} \n",
    "\n",
    "for target_domain in config.DOMAINS:\n",
    "    print(f\"==============================================================\")\n",
    "    print(f\"  STARTING LODO EXPERIMENT: Target Domain = {target_domain.upper()}\")\n",
    "    print(f\"==============================================================\")\n",
    "    \n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        root_dir=config.DATA_DIR, target_domain=target_domain,\n",
    "        all_domains=config.DOMAINS, batch_size=config.BATCH_SIZE, seed=config.SEED\n",
    "    )\n",
    "    \n",
    "    current_dropout_rates = list(np.random.choice(\n",
    "        config.DROPOUT_OPTIONS, config.NUM_HEADS, replace=False\n",
    "    ))\n",
    "    current_dummy_rate = np.random.choice(config.DROPOUT_OPTIONS)\n",
    "    \n",
    "    model = DistillationViT(\n",
    "        model_name=config.MODEL_NAME, num_classes=config.NUM_CLASSES,\n",
    "        dropout_rates=current_dropout_rates,\n",
    "        dummy_dropout_rate=current_dummy_rate\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(model.vit_backbone.parameters()) + list(model.heads.parameters()), \n",
    "        lr=config.LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.NUM_EPOCHS} ---\")\n",
    "        print(f\"Current Teacher Rates: { {f'head_{i+1}': rate for i, rate in enumerate(current_dropout_rates)} }\")\n",
    "        print(f\"Current Dummy Head Rate: {current_dummy_rate}\")\n",
    "\n",
    "        train_metrics = train_one_epoch(model, train_loader, optimizer, config.DEVICE)\n",
    "        val_metrics = evaluate(model, val_loader, config.DEVICE)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Summary:\")\n",
    "        print(f\"  Backbone Loss: {train_metrics['avg_backbone_loss']:.4f}\")\n",
    "        print(f\"  Dummy Diagnostic Loss: {train_metrics['avg_dummy_loss']:.4f}\") # NEW PRINT\n",
    "        print(f\"  Validation Loss: {val_metrics['avg_loss']:.4f}\")\n",
    "        print(f\"  Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        print(f\"  Epoch Training Accuracies (Teachers):\")\n",
    "        epoch_winner_head_name = max(train_metrics['head_accuracies'], key=train_metrics['head_accuracies'].get)\n",
    "        for name, acc in sorted(train_metrics['head_accuracies'].items()):\n",
    "            marker = \"<- EPOCH WINNER\" if name == epoch_winner_head_name else \"\"\n",
    "            print(f\"    {name}: {acc:.4f} {marker}\")\n",
    "        \n",
    "        # --- Store ALL metrics for plotting ---\n",
    "        history[\"backbone_loss\"].append(train_metrics['avg_backbone_loss'])\n",
    "        history[\"dummy_loss\"].append(train_metrics['avg_dummy_loss']) # NEW STORAGE\n",
    "        for i in range(config.NUM_HEADS):\n",
    "             history[f\"head_{i+1}_loss\"].append(train_metrics[f\"avg_head_{i+1}_loss\"]) # NEW STORAGE\n",
    "        history[\"val_loss\"].append(val_metrics['avg_loss'])\n",
    "        history[\"val_accuracy\"].append(val_metrics['accuracy'])\n",
    "\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            print(f\"  New best validation accuracy! Saving model state.\")\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        print(\"  Updating all dropout rates for next epoch...\")\n",
    "        current_dropout_rates = list(np.random.choice(config.DROPOUT_OPTIONS, config.NUM_HEADS, replace=False))\n",
    "        model.update_dropout_rates(current_dropout_rates)\n",
    "        \n",
    "        current_dummy_rate = np.random.choice(config.DROPOUT_OPTIONS)\n",
    "        model.update_dummy_dropout_rate(current_dummy_rate)\n",
    "\n",
    "    lodo_histories[target_domain] = history\n",
    "\n",
    "    print(\"\\nTraining complete. Loading best model for test evaluation...\")\n",
    "    model.load_state_dict(best_model_state)\n",
    "    test_metrics = evaluate(model, test_loader, config.DEVICE)\n",
    "    \n",
    "    print(f\"\\n--- RESULTS FOR TARGET DOMAIN: {target_domain.upper()} ---\")\n",
    "    print(f\"  Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    experiment_results.append({\n",
    "        \"target_domain\": target_domain,\n",
    "        \"test_accuracy\": test_metrics['accuracy'],\n",
    "        \"best_val_accuracy\": best_val_accuracy\n",
    "    })\n",
    "\n",
    "print(\"\\n\\nALL DIAGNOSTIC EMA LODO EXPERIMENTS COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 7: VISUALIZE COMPONENT LOSS CURVES (SUBPLOTS VERSION)\n",
    "# =================================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Visualizing Component Loss Curves (Subplots) ---\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a figure with a 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "# Flatten the 2x2 array of axes to make it easy to loop over\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define the colors and line styles to be consistent across all subplots\n",
    "colors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3'] \n",
    "\n",
    "# Loop through each domain's history and its corresponding subplot axis\n",
    "for i, (domain, history) in enumerate(lodo_histories.items()):\n",
    "    ax = axes[i]\n",
    "    epochs = range(1, config.NUM_EPOCHS + 1)\n",
    "    \n",
    "    # 1. Backbone Loss (Red Solid) - This is the winner's loss\n",
    "    ax.plot(epochs, history['backbone_loss'], 'r-', linewidth=3, label='Backbone Loss (Winner)')\n",
    "    \n",
    "    # 2. Dummy Head Diagnostic Loss (Blue Solid)\n",
    "    ax.plot(epochs, history['dummy_loss'], 'b-', linewidth=3, label='Dummy Head Loss')\n",
    "    \n",
    "    # 3. Validation Loss (Black Dotted)\n",
    "    ax.plot(epochs, history['val_loss'], 'k:', linewidth=2, label='Validation Loss')\n",
    "    \n",
    "    # 4-7. Individual Teacher Head Losses (Various Colored Dashed Lines)\n",
    "    for j in range(config.NUM_HEADS):\n",
    "        ax.plot(epochs, history[f'head_{j+1}_loss'], linestyle='--', color=colors[j], alpha=0.8, label=f'Head {j+1} Loss')\n",
    "\n",
    "    # --- Subplot Styling ---\n",
    "    ax.set_title(f'Target Domain: {domain.upper()}', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Epochs', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    ax.set_ylim(bottom=0) # Ensure y-axis starts at 0 for fair comparison\n",
    "\n",
    "# --- Overall Figure Styling ---\n",
    "# Create a single, shared legend for the entire figure\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.0), ncol=7, fontsize=14, fancybox=True)\n",
    "\n",
    "fig.suptitle('Full Training Loss Dynamics Across All LODO Experiments', fontsize=24, fontweight='bold', y=1.05)\n",
    "\n",
    "# Adjust layout to prevent titles and labels from overlapping\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 6: ANALYSIS & VISUALIZATION (with Dictionary Output)\n",
    "# =================================================================================\n",
    "# Now that all experiments are complete, we'll process the results\n",
    "# and create visualizations to understand the performance of our method.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# Add this magic command to ensure plots are displayed in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 6.1: DISPLAY RESULTS IN A TABLE\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"--- Final Experiment Results ---\")\n",
    "\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "column_order = [\n",
    "    \"target_domain\", \"test_accuracy\", \"best_val_accuracy\", \"num_epochs\",\n",
    "    \"batch_size\", \"learning_rate\", \"model_name\"\n",
    "]\n",
    "existing_columns = [col for col in column_order if col in results_df.columns]\n",
    "results_df = results_df[existing_columns]\n",
    "average_accuracy = results_df['test_accuracy'].mean()\n",
    "\n",
    "print(results_df.to_string())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Average Test Accuracy Across All Domains: {average_accuracy:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 6.2: VISUALIZE THE RESULTS\n",
    "# ---------------------------------------------------------------------------------\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    data=results_df, x='target_domain', y='test_accuracy', ax=ax, palette='viridis'\n",
    ")\n",
    "\n",
    "for index, row in results_df.iterrows():\n",
    "    ax.text(index, row['test_accuracy'] + 0.01, f\"{row['test_accuracy']:.2%}\",\n",
    "            color='black', ha=\"center\", fontsize=12)\n",
    "    \n",
    "ax.axhline(average_accuracy, ls='--', color='red', label=f'Average Accuracy ({average_accuracy:.2%})')\n",
    "\n",
    "ax.set_title('Model Performance on Unseen Target Domains (LODO)', fontsize=16, pad=20)\n",
    "ax.set_xlabel('Target (Unseen) Domain', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "### NEW SECTION ###\n",
    "# 6.3: GENERATE COPY-PASTE DICTIONARY FOR FINAL PLOTTING\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Dictionary for Final Plotting ---\")\n",
    "print(\"# Copy the dictionary below and paste it into your final analysis notebook.\")\n",
    "\n",
    "# Determine the variable name based on the notebook (you can adjust this)\n",
    "# For the baseline notebook, you'd want 'baseline_results'.\n",
    "# For the evolutionary notebook, you'd want 'evolutionary_results'.\n",
    "method_name = \"my_method_results\" # Generic name\n",
    "if \"baseline\" in os.getcwd(): # Simple check if 'baseline' is in the notebook path\n",
    "    method_name = \"baseline_results\"\n",
    "elif \"drop-out\" in os.getcwd():\n",
    "    method_name = \"evolutionary_results\"\n",
    "    \n",
    "# Extract the lists from the DataFrame\n",
    "domain_list = results_df['target_domain'].tolist()\n",
    "accuracy_list = [round(acc, 4) for acc in results_df['test_accuracy'].tolist()]\n",
    "\n",
    "# Print in the desired format\n",
    "print(f\"{method_name} = {{\")\n",
    "print(f\"    'target_domain': {domain_list},\")\n",
    "print(f\"    'test_accuracy': {accuracy_list}\")\n",
    "print(f\"}}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "print(\"\\n--- Experiment Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 7: COMPARATIVE ANALYSIS & VISUALIZATION (ACADEMIC STYLE - FINAL FIX)\n",
    "# =================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 7.1: COMBINE EXPERIMENT RESULTS ---\n",
    "baseline_results = {\n",
    "    'target_domain': ['art_painting', 'cartoon', 'photo', 'sketch'],\n",
    "    'test_accuracy': [0.8213, 0.7082, 0.9060, 0.5887]\n",
    "}\n",
    "# Using the results from your successful Option 4 run\n",
    "evolutionary_results = {\n",
    "    'target_domain': ['art_painting', 'cartoon', 'photo', 'sketch'],\n",
    "    'test_accuracy': [0.7993, 0.7381, 0.9587, 0.6149]\n",
    "}\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "baseline_df['method_name'] = 'Baseline'\n",
    "evolutionary_df = pd.DataFrame(evolutionary_results)\n",
    "evolutionary_df['method_name'] = 'Train All'\n",
    "combined_df = pd.concat([baseline_df, evolutionary_df])\n",
    "\n",
    "# --- 7.2: CREATE THE GROUPED BAR CHART (ROBUST VERSION) ---\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "custom_palette = {'Baseline': '#4B6A9A', 'Train All': '#66C2A5'}\n",
    "\n",
    "barplot = sns.barplot(\n",
    "    data=combined_df,\n",
    "    x='target_domain',\n",
    "    y='test_accuracy',\n",
    "    hue='method_name',\n",
    "    ax=ax,\n",
    "    palette=custom_palette,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "### THE FIX IS HERE ###\n",
    "# Use the robust 'containers' method to apply patterns correctly.\n",
    "\n",
    "# ax.containers[0] is the container for the first hue category (Baseline)\n",
    "# ax.containers[1] is the container for the second hue category (Evolutionary Dropout)\n",
    "\n",
    "# We want to add a pattern to the second container's bars.\n",
    "for bar in ax.containers[1]:\n",
    "    bar.set_hatch('..')\n",
    "\n",
    "# We also need to apply the pattern to the corresponding legend handle.\n",
    "# The legend handles are created in the same order.\n",
    "ax.legend_.legend_handles[1].set_hatch('..')\n",
    "\n",
    "# --- Add annotations (text on bars) ---\n",
    "for p in ax.patches:\n",
    "    if p.get_height() > 0:\n",
    "        ax.annotate(\n",
    "            f\"{p.get_height():.2%}\",\n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "            ha='center', va='center',\n",
    "            xytext=(0, 10),\n",
    "            textcoords='offset points',\n",
    "            fontsize=14,\n",
    "            fontweight='bold',\n",
    "            color='black'\n",
    "        )\n",
    "\n",
    "# --- Final plot styling ---\n",
    "ax.set_title('Model Comparison on Unseen Target Domains (LODO)', fontsize=22, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Target (Unseen) Domain', fontsize=18, fontweight='bold')\n",
    "ax.set_ylabel('Top-1 Test Accuracy (%)', fontsize=18, fontweight='bold')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.0%}'))\n",
    "\n",
    "legend = ax.get_legend()\n",
    "plt.setp(legend.get_title(), fontweight='bold')\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Print the final summary table ---\n",
    "avg_baseline = baseline_df['test_accuracy'].mean()\n",
    "avg_evolutionary = evolutionary_df['test_accuracy'].mean()\n",
    "print(\"\\n--- Average Performance Summary ---\")\n",
    "print(f\"Average Baseline Accuracy: {avg_baseline:.2%}\")\n",
    "print(f\"Average Evolutionary Dropout Accuracy: {avg_evolutionary:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8446344,
     "sourceId": 13323122,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
