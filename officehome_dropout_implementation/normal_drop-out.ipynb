{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 1: PROJECT SCAFFOLDING & CONFIGURATION\n",
    "# =================================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1.1: IMPORTS\n",
    "# All necessary libraries for the project.\n",
    "# ---------------------------------------------------------------------------------\n",
    "%matplotlib inline \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold # We might use this later for robustness\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1.2: CONFIGURATION CLASS\n",
    "# This class holds all hyperparameters and settings in one place.\n",
    "# ---------------------------------------------------------------------------------\n",
    "class Config:\n",
    "    # --- Data Paths and Domains ---\n",
    "    DATA_DIR = r\"D:\\Haseeb\\Datasets\\VLCS\" # Make sure this path is correct for your dataset\n",
    "    DOMAINS = [\"Caltech101\", \"LabelMe\", \"SUN09\", \"VOC2007\"]\n",
    "    \n",
    "    # --- Model & Architecture ---\n",
    "    MODEL_NAME = \"WinKawaks/vit-tiny-patch16-224\"\n",
    "    NUM_CLASSES = 5\n",
    "    ### CHANGE ###\n",
    "    NUM_HEADS = 4 \n",
    "    DROPOUT_OPTIONS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    \n",
    "    # --- Training Hyperparameters ---\n",
    "    BATCH_SIZE = 128\n",
    "    NUM_EPOCHS = 5\n",
    "    LEARNING_RATE = 1e-4\n",
    "    OPTIMIZER = \"AdamW\"\n",
    "    \n",
    "    # --- Hardware & Reproducibility ---\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 42\n",
    "\n",
    "# Instantiate the config\n",
    "config = Config()\n",
    "\n",
    "# Print out the configuration to verify\n",
    "print(\"--- Project Configuration ---\")\n",
    "for key, value in config.__class__.__dict__.items():\n",
    "    if not key.startswith('__'):\n",
    "        print(f\"{key}: {value}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1.3: RESULTS TRACKER\n",
    "# A list to store the final results from each LODO experiment run.\n",
    "# This will be converted to a DataFrame at the end for analysis.\n",
    "# ---------------------------------------------------------------------------------\n",
    "experiment_results = []\n",
    "\n",
    "print(\"\\nProject scaffolding is complete. Ready for Section 2: Data Loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 2: DATA LOADING & PREPROCESSING\n",
    "# =================================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 2.1: IMAGE TRANSFORMATIONS\n",
    "# Define the transformations for training (with augmentation) and validation/testing.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# The ViT model was pre-trained on images of size 224x224\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# The normalization values are standard for many pre-trained models\n",
    "# but it's good practice to use the ones specified by the model's authors if available.\n",
    "# For ViT, a simple (0.5, 0.5, 0.5) normalization is common.\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(), # A simple data augmentation technique\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "# 2.2: CUSTOM PACS DATASET CLASS\n",
    "# This class will read the images and labels from our specific folder structure.\n",
    "# ---------------------------------------------------------------------------------\n",
    "import random # Make sure to import the 'random' library at the top of the cell.\n",
    "\n",
    "class VLCSDataset(Dataset):\n",
    "    def __init__(self, root_dir, domains, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.domains = domains\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        ### CHANGE 2: Add a counter for corrupted images ###\n",
    "        self.corrupted_images_count = 0\n",
    "        \n",
    "        try:\n",
    "            self.classes = sorted(os.listdir(os.path.join(root_dir, domains[0])))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find domain folder at {os.path.join(root_dir, domains[0])}\")\n",
    "            print(\"Please ensure your DATA_DIR and DOMAINS in the Config class are correct.\")\n",
    "            raise\n",
    "\n",
    "        for domain in self.domains:\n",
    "            domain_path = os.path.join(self.root_dir, domain)\n",
    "            for class_name in self.classes:\n",
    "                class_path = os.path.join(domain_path, class_name)\n",
    "                if os.path.isdir(class_path):\n",
    "                    for img_name in os.listdir(class_path):\n",
    "                        self.image_paths.append(os.path.join(class_path, img_name))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.image_paths[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "        except (OSError, IOError) as e:\n",
    "            ### CHANGE 3: Increment the counter and replace with a random image ###\n",
    "            self.corrupted_images_count += 1\n",
    "            # You can uncomment the print statement below for verbose debugging if you want\n",
    "            # print(f\"Warning: Corrupted image at {img_path}. Replacing with random. Total corrupt: {self.corrupted_images_count}\")\n",
    "            \n",
    "            # Pick a new random index\n",
    "            new_idx = random.randint(0, len(self) - 1)\n",
    "            # Recursively call __getitem__ with the new index.\n",
    "            return self.__getitem__(new_idx)\n",
    "\n",
    "        # The transformation part remains the same\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 2.3: DATALOADER HELPER FUNCTION (NEW 80/20 SPLIT VERSION)\n",
    "# =================================================================================\n",
    "# Replace the old get_dataloaders function in Section 2 of BOTH notebooks with this.\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataloaders(root_dir, target_domain, all_domains, batch_size, seed):\n",
    "    \"\"\"\n",
    "    Creates dataloaders for a LODO split using an 80/20 split on the source domains.\n",
    "    \"\"\"\n",
    "    source_domains = [d for d in all_domains if d != target_domain]\n",
    "    \n",
    "    print(f\"--- Creating DataLoaders (80/20 Split Strategy) ---\")\n",
    "    print(f\"Target (Test) Domain: {target_domain}\")\n",
    "    print(f\"Source Domains for Train/Val: {source_domains}\")\n",
    "    \n",
    "    # 1. Create a single, large dataset by combining all source domains\n",
    "    source_dataset = VLCSDataset(\n",
    "        root_dir=root_dir, \n",
    "        domains=source_domains, \n",
    "        transform=data_transforms['train'] # Use training transforms for the whole source\n",
    "    )\n",
    "    \n",
    "    # We need to perform a stratified split to ensure the train and val sets\n",
    "    # have a similar distribution of classes.\n",
    "    indices = list(range(len(source_dataset)))\n",
    "    labels = source_dataset.labels\n",
    "    \n",
    "    # Use sklearn's train_test_split to get indices for an 80% train / 20% val split\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        indices, \n",
    "        test_size=0.2, \n",
    "        stratify=labels, \n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # 2. Create the training and validation subsets\n",
    "    train_subset = Subset(source_dataset, train_idx)\n",
    "    val_subset = Subset(source_dataset, val_idx)\n",
    "    \n",
    "    # Important: The validation subset should not use training augmentations (like RandomFlip).\n",
    "    # We create a new dataset object for validation with the correct transforms.\n",
    "    # This is a cleaner way to handle transforms for subsets.\n",
    "    val_dataset_clean = VLCSDataset(root_dir=root_dir, domains=source_domains, transform=data_transforms['val'])\n",
    "    val_subset_final = Subset(val_dataset_clean, val_idx)\n",
    "    \n",
    "    # 3. Create the test dataset from the full target domain\n",
    "    test_dataset = VLCSDataset(\n",
    "        root_dir=root_dir, \n",
    "        domains=[target_domain], \n",
    "        transform=data_transforms['val']\n",
    "    )\n",
    "\n",
    "    # 4. Create the DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_subset_final, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Source data size: {len(source_dataset)}\")\n",
    "    print(f\"  -> Training on: {len(train_subset)} images (80%)\")\n",
    "    print(f\"  -> Validating on: {len(val_subset_final)} images (20%)\")\n",
    "    print(f\"Testing on full '{target_domain}' domain: {len(test_dataset)} images\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 3: THE MODEL ARCHITECTURE\n",
    "# =================================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 3.1: CUSTOM ViT MODEL WITH EVOLUTIONARY HEADS\n",
    "# We define a class that wraps the ViT backbone and adds our 4 competing heads.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "class EvolutionaryViT(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rates: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name (string): The name of the pre-trained ViT model from Hugging Face.\n",
    "            num_classes (int): The number of output classes.\n",
    "            num_heads (int): The number of parallel classification heads.\n",
    "            dropout_rate (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        super(EvolutionaryViT, self).__init__()\n",
    "        \n",
    "        # 1. Load the pre-trained ViT backbone\n",
    "        # We use ViTModel, which gives us the feature extractor without the final classification layer.\n",
    "        self.vit_backbone = ViTModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Get the hidden size (feature dimension) from the model's config\n",
    "        hidden_dim = self.vit_backbone.config.hidden_size\n",
    "        \n",
    "        # 2. Create the list of competing heads\n",
    "        # We use nn.ModuleList, which is the proper way to hold a list of PyTorch modules.\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Dropout(p=rate), # Use the specific rate for this head\n",
    "                nn.Linear(hidden_dim, num_classes)\n",
    "            ) for rate in dropout_rates\n",
    "        ])\n",
    "\n",
    "    def update_dropout_rates(self, new_rates: list):\n",
    "        \"\"\"\n",
    "        Updates the dropout probability for each head.\n",
    "        This allows us to change the rates between epochs without re-creating the model.\n",
    "        \"\"\"\n",
    "        for i, head in enumerate(self.heads):\n",
    "            # nn.Sequential has layers indexed. 0 is Dropout, 1 is Linear.\n",
    "            head[0].p = new_rates[i]\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        \"\"\"\n",
    "        # 1. Get features from the backbone\n",
    "        # The output is a dictionary-like object. We want the 'last_hidden_state'.\n",
    "        outputs = self.vit_backbone(pixel_values=images)\n",
    "        \n",
    "        # For ViT, the feature representation for the entire image is the output\n",
    "        # corresponding to the special [CLS] token, which is the first one.\n",
    "        # Shape: (batch_size, sequence_length, hidden_dim) -> (batch_size, hidden_dim)\n",
    "        feature_vector_z = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # 2. Pass the feature vector through all heads\n",
    "        head_outputs = {}\n",
    "        for i, head in enumerate(self.heads):\n",
    "            head_outputs[f'head_{i+1}'] = head(feature_vector_z)\n",
    "            \n",
    "        return head_outputs\n",
    "\n",
    "# --- Let's test it to make sure it works ---\n",
    "print(\"\\nModel architecture seems correct. Ready for Section 4: Training and Evaluation Logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 4: TRAINING & EVALUATION LOGIC\n",
    "# =================================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 4.1: TRAIN_ONE_EPOCH FUNCTION\n",
    "# This function handles the custom \"survival of the fittest\" training loop for one epoch.\n",
    "# ---------------------------------------------------------------------------------\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    \n",
    "    total_winner_loss = 0.0\n",
    "    # We still track accuracy over the epoch to see general trends\n",
    "    head_correct_preds = defaultdict(int)\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training Epoch\", leave=False)\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # --- Per-Batch Tournament ---\n",
    "        # 1. Forward pass\n",
    "        head_outputs = model(images)\n",
    "        \n",
    "        # 2. Find the winner FOR THIS BATCH\n",
    "        batch_accuracies = {}\n",
    "        batch_losses = {}\n",
    "        for head_name, logits in head_outputs.items():\n",
    "            loss = criterion(logits, labels)\n",
    "            batch_losses[head_name] = loss\n",
    "            \n",
    "            _, preds = torch.max(logits, 1)\n",
    "            correct = torch.sum(preds == labels).item()\n",
    "            batch_accuracies[head_name] = correct / labels.size(0)\n",
    "\n",
    "            # Update epoch-level stats for logging\n",
    "            head_correct_preds[head_name] += correct\n",
    "            \n",
    "        total_samples += labels.size(0)\n",
    "            \n",
    "        winner_head_name = max(batch_accuracies, key=batch_accuracies.get)\n",
    "        \n",
    "        # 3. Backpropagate from the batch winner's loss\n",
    "        winner_loss = batch_losses[winner_head_name]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        winner_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_winner_loss += winner_loss.item()\n",
    "\n",
    "    # --- End of Epoch ---\n",
    "    final_head_accuracies = {name: (correct / total_samples) for name, correct in head_correct_preds.items()}\n",
    "    \n",
    "    return {\n",
    "        \"avg_winner_loss\": total_winner_loss / len(train_loader),\n",
    "        \"head_accuracies\": final_head_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 4: TRAINING & EVALUATION LOGIC (ADVANCED STRATEGY)\n",
    "# =================================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 4.1: TRAIN_ONE_EPOCH FUNCTION\n",
    "# (This function is correct and does not need to change)\n",
    "# ---------------------------------------------------------------------------------\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_winner_loss = 0.0\n",
    "    head_correct_preds = defaultdict(int)\n",
    "    total_samples = 0\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training Epoch\", leave=False)\n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        head_outputs = model(images)\n",
    "        batch_accuracies = {}\n",
    "        batch_losses = {}\n",
    "        for head_name, logits in head_outputs.items():\n",
    "            loss = criterion(logits, labels)\n",
    "            batch_losses[head_name] = loss\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            correct = torch.sum(preds == labels).item()\n",
    "            batch_accuracies[head_name] = correct / labels.size(0)\n",
    "            head_correct_preds[head_name] += correct\n",
    "        total_samples += labels.size(0)\n",
    "        winner_head_name = max(batch_accuracies, key=batch_accuracies.get)\n",
    "        winner_loss = batch_losses[winner_head_name]\n",
    "        optimizer.zero_grad()\n",
    "        winner_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_winner_loss += winner_loss.item()\n",
    "    final_head_accuracies = {name: (correct / total_samples) for name, correct in head_correct_preds.items()}\n",
    "    return {\n",
    "        \"avg_winner_loss\": total_winner_loss / len(train_loader),\n",
    "        \"head_accuracies\": final_head_accuracies\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 4.2: EVALUATE FUNCTION\n",
    "# (This function is correct and does not need to change)\n",
    "# ---------------------------------------------------------------------------------\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total_samples += labels.size(0)\n",
    "            head_outputs = model(images)\n",
    "            all_logits = torch.stack(list(head_outputs.values()))\n",
    "            ensembled_logits = torch.mean(all_logits, dim=0)\n",
    "            loss = criterion(ensembled_logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(ensembled_logits, 1)\n",
    "            correct_preds += torch.sum(preds == labels).item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_preds / total_samples\n",
    "    return {\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "# --- Quick Test of the Functions (Optional but Recommended) ---\n",
    "### CHANGE ###\n",
    "# The test block is now updated to work with the new model __init__\n",
    "# and the new 80/20 get_dataloaders function.\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"Running a quick test of the training and evaluation functions...\")\n",
    "\n",
    "# We need some dataloaders for the test\n",
    "target_domain_test = config.DOMAINS[3] # \"sketch\"\n",
    "train_loader_test, val_loader_test, _ = get_dataloaders(\n",
    "    root_dir=config.DATA_DIR,\n",
    "    target_domain=target_domain_test,\n",
    "    all_domains=config.DOMAINS,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    seed=config.SEED # Pass the seed\n",
    ")\n",
    "\n",
    "# 1. Generate an initial list of dropout rates for the test\n",
    "initial_dropout_rates = list(np.random.choice(\n",
    "    config.DROPOUT_OPTIONS, \n",
    "    config.NUM_HEADS, \n",
    "    replace=False\n",
    "))\n",
    "print(f\"Test model initial dropout rates: {initial_dropout_rates}\")\n",
    "\n",
    "# 2. Instantiate the model using the LIST of rates\n",
    "test_model = EvolutionaryViT(\n",
    "    model_name=config.MODEL_NAME,\n",
    "    num_classes=config.NUM_CLASSES,\n",
    "    dropout_rates=initial_dropout_rates # Pass the list here\n",
    ").to(config.DEVICE)\n",
    "\n",
    "test_optimizer = torch.optim.AdamW(test_model.parameters(), lr=config.LEARNING_RATE)\n",
    "test_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Run one training epoch\n",
    "train_metrics = train_one_epoch(test_model, train_loader_test, test_optimizer, test_criterion, config.DEVICE)\n",
    "print(\"\\n--- One Training Epoch Test ---\")\n",
    "print(f\"Average Winner Loss: {train_metrics['avg_winner_loss']:.4f}\") \n",
    "print(\"Final Head Accuracies for the Epoch:\")\n",
    "for name, acc in train_metrics['head_accuracies'].items():\n",
    "    print(f\"  {name}: {acc:.4f}\")\n",
    "\n",
    "# Run one evaluation pass\n",
    "eval_metrics = evaluate(test_model, val_loader_test, test_criterion, config.DEVICE)\n",
    "print(\"\\n--- One Evaluation Pass Test ---\")\n",
    "print(f\"Ensembled Validation Loss: {eval_metrics['avg_loss']:.4f}\")\n",
    "print(f\"Ensembled Validation Accuracy: {eval_metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nTraining and evaluation logic seems correct. Ready for Section 5: The Main Experiment Loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 5: THE MAIN EXPERIMENT LOOP (ADVANCED STRATEGY)\n",
    "# =================================================================================\n",
    "# This version implements the \"Winner-Stays, Losers-Re-roll\" dropout strategy.\n",
    "# The dropout rates are now dynamic and adapt based on epoch performance.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# A fresh copy of the config to ensure we start clean\n",
    "config = Config()\n",
    "\n",
    "# Loop over each domain to set it as the target domain once\n",
    "for target_domain in config.DOMAINS:\n",
    "    print(f\"==============================================================\")\n",
    "    print(f\"  STARTING LODO EXPERIMENT: Target Domain = {target_domain.upper()}\")\n",
    "    print(f\"==============================================================\")\n",
    "    \n",
    "    # --- 1. Setup for this specific LODO run ---\n",
    "    # Get the specific data loaders for this train/test split\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        root_dir=config.DATA_DIR,\n",
    "        target_domain=target_domain,\n",
    "        all_domains=config.DOMAINS,\n",
    "        batch_size=config.BATCH_SIZE, seed=config.SEED\n",
    "    )\n",
    "    \n",
    "    # Initialize the first set of random dropout rates for the competing heads.\n",
    "    # np.random.choice ensures we get unique rates if possible.\n",
    "    current_dropout_rates = list(np.random.choice(\n",
    "        config.DROPOUT_OPTIONS, \n",
    "        config.NUM_HEADS, \n",
    "        replace=False # Tries to pick unique rates\n",
    "    ))\n",
    "    \n",
    "    # Initialize a fresh model with the starting dropout rates\n",
    "    model = EvolutionaryViT(\n",
    "        model_name=config.MODEL_NAME,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        dropout_rates=current_dropout_rates # Pass the list of rates\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    # Initialize a fresh optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # --- 2. Training Loop for this LODO run ---\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.NUM_EPOCHS} ---\")\n",
    "        print(f\"Current Dropout Rates: { {f'head_{i+1}': rate for i, rate in enumerate(current_dropout_rates)} }\")\n",
    "        \n",
    "        # Train for one epoch using the per-batch winner selection\n",
    "        train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, config.DEVICE)\n",
    "        \n",
    "        # Evaluate on the validation set to check progress and save the best model\n",
    "        val_metrics = evaluate(model, val_loader, criterion, config.DEVICE)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Avg Winner Loss: {train_metrics['avg_winner_loss']:.4f}\")\n",
    "        print(f\"  Validation Loss: {val_metrics['avg_loss']:.4f}\")\n",
    "        print(f\"  Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # Check if this is the best model so far based on validation performance\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            print(f\"  New best validation accuracy! Saving model state.\")\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # --- Adaptive Dropout Logic for the NEXT epoch ---\n",
    "        # Find the head that had the best OVERALL accuracy during this epoch\n",
    "        epoch_winner_head_name = max(train_metrics['head_accuracies'], key=train_metrics['head_accuracies'].get)\n",
    "        epoch_winner_index = int(epoch_winner_head_name.split('_')[-1]) - 1\n",
    "        \n",
    "        print(f\"  Epoch Training Accuracies:\")\n",
    "        for name, acc in sorted(train_metrics['head_accuracies'].items()):\n",
    "            marker = \"<- WINNER\" if name == epoch_winner_head_name else \"\"\n",
    "            print(f\"    {name}: {acc:.4f} {marker}\")\n",
    "        \n",
    "        # Keep the winner's dropout rate for the next epoch\n",
    "        winner_rate = current_dropout_rates[epoch_winner_index]\n",
    "        \n",
    "        # Generate a new set of random rates for all heads\n",
    "        new_random_rates = list(np.random.choice(config.DROPOUT_OPTIONS, config.NUM_HEADS, replace=False))\n",
    "        \n",
    "        # \"Exploitation\": Overwrite the winner's slot with its successful rate\n",
    "        new_random_rates[epoch_winner_index] = winner_rate\n",
    "        \n",
    "        # \"Exploration\": The other heads get new random rates\n",
    "        current_dropout_rates = new_random_rates\n",
    "        \n",
    "        # Update the model in-place with the new dropout rates\n",
    "        model.update_dropout_rates(current_dropout_rates)\n",
    "            \n",
    "    # --- 3. Final Evaluation for this LODO run ---\n",
    "    print(\"\\nTraining complete for this LODO split.\")\n",
    "    print(\"Loading best model state and evaluating on the TEST set...\")\n",
    "    \n",
    "    # Load the best performing model based on validation accuracy\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Evaluate on the unseen target domain (the test set)\n",
    "    test_metrics = evaluate(model, test_loader, criterion, config.DEVICE)\n",
    "    \n",
    "    print(f\"\\n--- RESULTS FOR TARGET DOMAIN: {target_domain.upper()} ---\")\n",
    "    print(f\"  Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    \n",
    "    # --- 4. Store the final results ---\n",
    "    experiment_results.append({\n",
    "        \"target_domain\": target_domain,\n",
    "        \"source_domains\": [d for d in config.DOMAINS if d != target_domain],\n",
    "        \"test_accuracy\": test_metrics['accuracy'],\n",
    "        \"best_val_accuracy\": best_val_accuracy,\n",
    "        \"model_name\": config.MODEL_NAME,\n",
    "        \"num_epochs\": config.NUM_EPOCHS,\n",
    "        \"batch_size\": config.BATCH_SIZE,\n",
    "        \"learning_rate\": config.LEARNING_RATE\n",
    "    })\n",
    "\n",
    "print(\"\\n\\n==============================================================\")\n",
    "print(\"          ALL ADAPTIVE DROPOUT LODO EXPERIMENTS COMPLETE\")\n",
    "print(\"==============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 6: ANALYSIS & VISUALIZATION (with Dictionary Output)\n",
    "# =================================================================================\n",
    "# Now that all experiments are complete, we'll process the results\n",
    "# and create visualizations to understand the performance of our method.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# Add this magic command to ensure plots are displayed in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 6.1: DISPLAY RESULTS IN A TABLE\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"--- Final Experiment Results ---\")\n",
    "\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "column_order = [\n",
    "    \"target_domain\", \"test_accuracy\", \"best_val_accuracy\", \"num_epochs\",\n",
    "    \"batch_size\", \"learning_rate\", \"model_name\"\n",
    "]\n",
    "existing_columns = [col for col in column_order if col in results_df.columns]\n",
    "results_df = results_df[existing_columns]\n",
    "average_accuracy = results_df['test_accuracy'].mean()\n",
    "\n",
    "print(results_df.to_string())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Average Test Accuracy Across All Domains: {average_accuracy:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 6.2: VISUALIZE THE RESULTS\n",
    "# ---------------------------------------------------------------------------------\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    data=results_df, x='target_domain', y='test_accuracy', ax=ax, palette='viridis'\n",
    ")\n",
    "\n",
    "for index, row in results_df.iterrows():\n",
    "    ax.text(index, row['test_accuracy'] + 0.01, f\"{row['test_accuracy']:.2%}\",\n",
    "            color='black', ha=\"center\", fontsize=12)\n",
    "    \n",
    "ax.axhline(average_accuracy, ls='--', color='red', label=f'Average Accuracy ({average_accuracy:.2%})')\n",
    "\n",
    "ax.set_title('Model Performance on Unseen Target Domains (LODO)', fontsize=16, pad=20)\n",
    "ax.set_xlabel('Target (Unseen) Domain', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "### NEW SECTION ###\n",
    "# 6.3: GENERATE COPY-PASTE DICTIONARY FOR FINAL PLOTTING\n",
    "# ---------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Dictionary for Final Plotting ---\")\n",
    "print(\"# Copy the dictionary below and paste it into your final analysis notebook.\")\n",
    "\n",
    "# Determine the variable name based on the notebook (you can adjust this)\n",
    "# For the baseline notebook, you'd want 'baseline_results'.\n",
    "# For the evolutionary notebook, you'd want 'evolutionary_results'.\n",
    "method_name = \"my_method_results\" # Generic name\n",
    "if \"baseline\" in os.getcwd(): # Simple check if 'baseline' is in the notebook path\n",
    "    method_name = \"baseline_results\"\n",
    "elif \"drop-out\" in os.getcwd():\n",
    "    method_name = \"evolutionary_results\"\n",
    "    \n",
    "# Extract the lists from the DataFrame\n",
    "domain_list = results_df['target_domain'].tolist()\n",
    "accuracy_list = [round(acc, 4) for acc in results_df['test_accuracy'].tolist()]\n",
    "\n",
    "# Print in the desired format\n",
    "print(f\"{method_name} = {{\")\n",
    "print(f\"    'target_domain': {domain_list},\")\n",
    "print(f\"    'test_accuracy': {accuracy_list}\")\n",
    "print(f\"}}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 7: COMPARATIVE ANALYSIS & VISUALIZATION (ACADEMIC STYLE - FINAL FIX)\n",
    "# =================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 7.1: COMBINE EXPERIMENT RESULTS ---\n",
    "baseline_results = {\n",
    "    'target_domain': ['art_painting', 'cartoon', 'photo', 'sketch'],\n",
    "    'test_accuracy': [0.9625, 0.661, 0.766, 0.7693]\n",
    "}\n",
    "# Using the results from your successful Option 4 run\n",
    "evolutionary_results = {\n",
    "    'target_domain': ['art_painting', 'cartoon', 'photo', 'sketch'],\n",
    "    'test_accuracy': [0.9484, 0.6704, 0.7236, 0.742]\n",
    "}\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "baseline_df['method_name'] = 'Baseline'\n",
    "evolutionary_df = pd.DataFrame(evolutionary_results)\n",
    "evolutionary_df['method_name'] = 'Evolutionary Dropout'\n",
    "combined_df = pd.concat([baseline_df, evolutionary_df])\n",
    "\n",
    "# --- 7.2: CREATE THE GROUPED BAR CHART (ROBUST VERSION) ---\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "custom_palette = {'Baseline': '#4B6A9A', 'Evolutionary Dropout': '#66C2A5'}\n",
    "\n",
    "barplot = sns.barplot(\n",
    "    data=combined_df,\n",
    "    x='target_domain',\n",
    "    y='test_accuracy',\n",
    "    hue='method_name',\n",
    "    ax=ax,\n",
    "    palette=custom_palette,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "### THE FIX IS HERE ###\n",
    "# Use the robust 'containers' method to apply patterns correctly.\n",
    "\n",
    "# ax.containers[0] is the container for the first hue category (Baseline)\n",
    "# ax.containers[1] is the container for the second hue category (Evolutionary Dropout)\n",
    "\n",
    "# We want to add a pattern to the second container's bars.\n",
    "for bar in ax.containers[1]:\n",
    "    bar.set_hatch('..')\n",
    "\n",
    "# We also need to apply the pattern to the corresponding legend handle.\n",
    "# The legend handles are created in the same order.\n",
    "ax.legend_.legend_handles[1].set_hatch('..')\n",
    "\n",
    "# --- Add annotations (text on bars) ---\n",
    "for p in ax.patches:\n",
    "    if p.get_height() > 0:\n",
    "        ax.annotate(\n",
    "            f\"{p.get_height():.2%}\",\n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "            ha='center', va='center',\n",
    "            xytext=(0, 10),\n",
    "            textcoords='offset points',\n",
    "            fontsize=14,\n",
    "            fontweight='bold',\n",
    "            color='black'\n",
    "        )\n",
    "\n",
    "# --- Final plot styling ---\n",
    "ax.set_title('Model Comparison on Unseen Target Domains (LODO)', fontsize=22, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Target (Unseen) Domain', fontsize=18, fontweight='bold')\n",
    "ax.set_ylabel('Top-1 Test Accuracy (%)', fontsize=18, fontweight='bold')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.0%}'))\n",
    "\n",
    "legend = ax.get_legend()\n",
    "plt.setp(legend.get_title(), fontweight='bold')\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Print the final summary table ---\n",
    "avg_baseline = baseline_df['test_accuracy'].mean()\n",
    "avg_evolutionary = evolutionary_df['test_accuracy'].mean()\n",
    "print(\"\\n--- Average Performance Summary ---\")\n",
    "print(f\"Average Baseline Accuracy: {avg_baseline:.2%}\")\n",
    "print(f\"Average Evolutionary Dropout Accuracy: {avg_evolutionary:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SECTION 8: GRAND COMPARATIVE ANALYSIS & VISUALIZATION (FINAL FIXED LEGEND)\n",
    "# =================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 8.1: COMBINE ALL EXPERIMENT RESULTS ---\n",
    "baseline_results = {\n",
    "    'target_domain': ['Caltech101', 'LabelMe', 'SUN09', 'VOC2007'],\n",
    "    'test_accuracy': [0.9795, 0.677, 0.7693, 0.7725]\n",
    "}\n",
    "wta_dropout_results = {\n",
    "    'target_domain': ['Caltech101', 'LabelMe', 'SUN09', 'VOC2007'],\n",
    "    'test_accuracy': [0.8947, 0.5644, 0.6313, 0.7011]\n",
    "}\n",
    "train_all_results = {\n",
    "    'target_domain': ['Caltech101', 'LabelMe', 'SUN09', 'VOC2007'],\n",
    "    'test_accuracy': [0.9675, 0.6634, 0.7639, 0.7518]\n",
    "}\n",
    "shared_head_results = {\n",
    "    'target_domain': ['Caltech101', 'LabelMe', 'SUN09', 'VOC2007'],\n",
    "    'test_accuracy': [0.9413, 0.6604, 0.777, 0.7435]\n",
    "}\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results); baseline_df['Method'] = 'A: Baseline'\n",
    "wta_df = pd.DataFrame(wta_dropout_results); wta_df['Method'] = 'B: Winner-Take-All'\n",
    "train_all_df = pd.DataFrame(train_all_results); train_all_df['Method'] = 'C: Train All Heads'\n",
    "shared_head_df = pd.DataFrame(shared_head_results); shared_head_df['Method'] = 'D: Shared Head'\n",
    "combined_df = pd.concat([baseline_df, wta_df, train_all_df, shared_head_df])\n",
    "\n",
    "# --- 8.2: CREATE THE GRAND GROUPED BAR CHART ---\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "fig, ax = plt.subplots(figsize=(18, 9))\n",
    "\n",
    "custom_palette = {\n",
    "    'A: Baseline': '#4B6A9A',\n",
    "    'B: Winner-Take-All': '#DB845B',\n",
    "    'C: Train All Heads': '#92B56F',\n",
    "    'D: Shared Head': '#66C2A5'\n",
    "}\n",
    "\n",
    "barplot = sns.barplot(\n",
    "    data=combined_df, x='target_domain', y='test_accuracy', hue='Method',\n",
    "    ax=ax, palette=custom_palette, edgecolor='black'\n",
    ")\n",
    "\n",
    "# Apply hatching patterns\n",
    "patterns = ['', '..', '//', 'xx']\n",
    "for i, container in enumerate(ax.containers):\n",
    "    for bar in container:\n",
    "        bar.set_hatch(patterns[i])\n",
    "\n",
    "# Add annotations on bars\n",
    "for p in ax.patches:\n",
    "    if p.get_height() > 0:\n",
    "        ax.annotate(f\"{p.get_height():.2%}\",\n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 9),\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=12, fontweight='bold', color='black')\n",
    "\n",
    "# --- Fix legend properly ---\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, labels,\n",
    "                   title='Method',\n",
    "                   loc='upper center',\n",
    "                   bbox_to_anchor=(0.5, 1),\n",
    "                   ncol=4,\n",
    "                   frameon=False,\n",
    "                   fontsize=14,\n",
    "                   title_fontsize=16)\n",
    "\n",
    "# Apply hatching to legend handles\n",
    "for i, handle in enumerate(legend.legend_handles):\n",
    "    handle.set_hatch(patterns[i])\n",
    "\n",
    "plt.setp(legend.get_title(), fontweight='bold')\n",
    "\n",
    "# --- Final styling ---\n",
    "ax.set_title('Comparison of All Methods on Unseen Target Domains (LODO)',\n",
    "             fontsize=24, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Target (Unseen) Domain', fontsize=18, fontweight='bold', labelpad=15)\n",
    "ax.set_ylabel('Top-1 Test Accuracy (%)', fontsize=18, fontweight='bold', labelpad=15)\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.0%}'))\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.show()\n",
    "\n",
    "# --- Summary table ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- Average Performance Summary ---\")\n",
    "print(f\"A: Average Baseline Accuracy: {baseline_df['test_accuracy'].mean():.2%}\")\n",
    "print(f\"B: Average Winner-Take-All Accuracy: {wta_df['test_accuracy'].mean():.2%}\")\n",
    "print(f\"C: Average Train All Heads Accuracy: {train_all_df['test_accuracy'].mean():.2%}\")\n",
    "print(f\"D: Average Shared Head Accuracy: {shared_head_df['test_accuracy'].mean():.2%}\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8446344,
     "sourceId": 13323122,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
